{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1oBuInjA0VXRNnlr6zlJs1XIWcFP00FvM",
      "authorship_tag": "ABX9TyPLWYMk+Gm83AUEOlg+qNg9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyasu-taye/tdn_complaince_detection_models/blob/main/aiobackup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQfD7V9vv3P3",
        "outputId": "446a4413-ae10-40f3-de90-57b90b0c1a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Script ready. Use --train <model_name> or --serve to launch the UI.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "import datetime\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Install gensim if not already present\n",
        "!pip install gensim\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
        "                                     Dense, Dropout, concatenate, LSTM, Bidirectional)\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# gensim FastText (real)\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Gradio\n",
        "import gradio as gr\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration (user-specified)\n",
        "# -----------------------------\n",
        "ROOT_MODEL_DIR = \"/content/drive/MyDrive/10models_aio_back\"\n",
        "CSV_FALLBACK = \"/content/drive/MyDrive/output_8_1_M_1_balanced_utf8.csv\"\n",
        "\n",
        "# Paths for FastText storage (user-specified)\n",
        "FT_GENSIM_DIR = \"/content/drive/MyDrive/10models_aio/fasttest_gensim_back\"\n",
        "FT_KERAS_DIR = \"/content/drive/MyDrive/10models_aio/fasttest_keras_back\"\n",
        "\n",
        "MODEL_NAMES = [\n",
        "    \"cnn_word\",\n",
        "    \"cnn_char\",\n",
        "    \"cnn_combined\",\n",
        "    \"cnn_fasttext_keras\",\n",
        "    \"cnn_fasttext_gensim\",\n",
        "    \"rnn_word\",\n",
        "    \"rnn_char\",\n",
        "    \"rnn_combined\",\n",
        "    \"rnn_fasttext_keras\",\n",
        "    \"rnn_fasttext_gensim\"\n",
        "]\n",
        "\n",
        "CFG = {\n",
        "    \"word\": {\"max_words\": 20000, \"max_len\": 25, \"embedding_dim\": 100},\n",
        "    \"char\": {\"max_chars\": 200, \"vocab_size\": 200, \"embedding_dim\": 64},\n",
        "    \"cnn\": {\"filters\": [2, 3, 4], \"num_filters\": 128, \"dropout\": 0.5},\n",
        "    \"rnn\": {\"rnn_units\": 128, \"dropout\": 0.5},\n",
        "    \"training\": {\"batch_size\": 64, \"epochs\": 2, \"validation_split\": 0.15},\n",
        "    # gensim FastText (real)\n",
        "    \"fasttext_gensim\": {\"vector_size\": 100, \"window\": 5, \"min_count\": 1, \"workers\": 4, \"epochs\": 10, \"sg\": 1},\n",
        "    # Keras-style fasttext embedding (trainable embedding layer, optional subword-like handling could be added)\n",
        "    \"fasttext_keras\": {\"embedding_dim\": 100}\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Ensure directories\n",
        "# -----------------------------\n",
        "def ensure_dirs():\n",
        "    os.makedirs(ROOT_MODEL_DIR, exist_ok=True)\n",
        "    os.makedirs(FT_GENSIM_DIR, exist_ok=True)\n",
        "    os.makedirs(FT_KERAS_DIR, exist_ok=True)\n",
        "    for mn in MODEL_NAMES:\n",
        "        os.makedirs(os.path.join(ROOT_MODEL_DIR, mn), exist_ok=True)\n",
        "\n",
        "ensure_dirs()\n",
        "\n",
        "# -----------------------------\n",
        "# Paths & helpers\n",
        "# -----------------------------\n",
        "def model_folder(model_name):\n",
        "    p = os.path.join(ROOT_MODEL_DIR, model_name)\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def model_paths(model_name):\n",
        "    base = model_folder(model_name)\n",
        "    return {\n",
        "        \"base\": base,\n",
        "        \"model_best\": os.path.join(base, \"model_best.h5\"),\n",
        "        \"model_epoch_pattern\": os.path.join(base, \"model_epoch-{epoch:02d}-val_loss-{val_loss:.4f}.h5\"),\n",
        "        \"word_tokenizer\": os.path.join(base, \"word_tokenizer.json\"),\n",
        "        \"char_tokenizer\": os.path.join(base, \"char_tokenizer.json\"),\n",
        "        \"classes\": os.path.join(base, \"classes.npy\"),\n",
        "        \"fasttext_gensim\": os.path.join(FT_GENSIM_DIR, f\"{model_name}_fasttext.model\"),\n",
        "        \"fasttext_keras\": os.path.join(FT_KERAS_DIR, f\"{model_name}_ft_keras.json\"),\n",
        "        \"training_state\": os.path.join(base, \"training_state.json\"),\n",
        "        \"train_log\": os.path.join(base, \"train.log\")\n",
        "    }\n",
        "\n",
        "def save_json_atomic(obj, path):\n",
        "    tmp = path + \".tmp\"\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def load_json(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def append_log(model_name, msg):\n",
        "    p = model_paths(model_name)[\"train_log\"]\n",
        "    ts = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
        "    with open(p, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"[{ts}] {msg}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Training state (resume)\n",
        "# -----------------------------\n",
        "def load_training_state(model_name):\n",
        "    p = model_paths(model_name)[\"training_state\"]\n",
        "    if os.path.exists(p):\n",
        "        try:\n",
        "            return load_json(p)\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def save_training_state(model_name, state):\n",
        "    p = model_paths(model_name)[\"training_state\"]\n",
        "    save_json_atomic(state, p)\n",
        "\n",
        "class EpochCheckpointCallback(Callback):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        state = load_training_state(self.model_name) or {}\n",
        "        if \"phases\" not in state:\n",
        "            state[\"phases\"] = {}\n",
        "        state[\"phases\"][self.model_name] = {\n",
        "            \"last_completed_epoch\": int(epoch) + 1,\n",
        "            \"updated_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"logs\": (logs or {})\n",
        "        }\n",
        "        save_training_state(self.model_name, state)\n",
        "\n",
        "# -----------------------------\n",
        "# Data loading & preprocessing\n",
        "# -----------------------------\n",
        "def simple_clean_text(text):\n",
        "    text = str(text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def load_data(csv_path=\"\"):\n",
        "    if csv_path and os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "    elif os.path.exists(CSV_FALLBACK):\n",
        "        df = pd.read_csv(CSV_FALLBACK)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"CSV not found at {csv_path} or fallback {CSV_FALLBACK}\")\n",
        "\n",
        "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
        "    if \"trade_name\" not in df.columns or \"reason\" not in df.columns:\n",
        "        raise ValueError(\"CSV must contain 'trade_name' and 'reason' columns\")\n",
        "    df = df[[\"trade_name\", \"reason\"]].dropna()\n",
        "    df[\"trade_name\"] = df[\"trade_name\"].astype(str).apply(simple_clean_text)\n",
        "    df[\"reason\"] = df[\"reason\"].astype(str).str.strip()\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenizers & char mapping\n",
        "# -----------------------------\n",
        "def build_word_tokenizer(texts, max_words):\n",
        "    tok = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "    tok.fit_on_texts(texts)\n",
        "    return tok\n",
        "\n",
        "def save_tokenizer_json(tokenizer, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(tokenizer.to_json())\n",
        "\n",
        "def load_tokenizer_json(path):\n",
        "    from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return tokenizer_from_json(f.read())\n",
        "\n",
        "def build_char_tokenizer(texts, max_vocab=None):\n",
        "    chars = set()\n",
        "    for s in texts:\n",
        "        for ch in s:\n",
        "            chars.add(ch)\n",
        "    chars = sorted(chars)\n",
        "    if max_vocab is not None:\n",
        "        chars = chars[:max_vocab-2]\n",
        "    char_to_index = {ch: idx+2 for idx, ch in enumerate(chars)}\n",
        "    char_to_index[\"<PAD>\"] = 0\n",
        "    char_to_index[\"<OOV>\"] = 1\n",
        "    return char_to_index\n",
        "\n",
        "def save_char_tokenizer(char_map, vocab_size, path):\n",
        "    save_json_atomic({\"char_to_index\": char_map, \"vocab_size\": vocab_size}, path)\n",
        "\n",
        "def load_char_tokenizer(path):\n",
        "    data = load_json(path)\n",
        "    return data[\"char_to_index\"], data.get(\"vocab_size\", max(data[\"char_to_index\"].values()) + 1)\n",
        "\n",
        "def texts_to_char_sequences(texts, char_to_index, max_len):\n",
        "    seqs = []\n",
        "    pad = char_to_index.get(\"<PAD>\", 0)\n",
        "    oov = char_to_index.get(\"<OOV>\", 1)\n",
        "    for s in texts:\n",
        "        arr = [char_to_index.get(ch, oov) for ch in s]\n",
        "        if len(arr) < max_len:\n",
        "            arr = arr + [pad] * (max_len - len(arr))\n",
        "        else:\n",
        "            arr = arr[:max_len]\n",
        "        seqs.append(arr)\n",
        "    return np.array(seqs, dtype=np.int32)\n",
        "\n",
        "# -----------------------------\n",
        "# FastText (gensim) helpers\n",
        "# -----------------------------\n",
        "def tokenize_for_fasttext(text):\n",
        "    return simple_preprocess(text, deacc=False)\n",
        "\n",
        "# def train_fasttext_gensim(sentences, path, ft_conf):\n",
        "#     if not sentences or len(sentences) == 0:\n",
        "#         raise ValueError(\"FastText requires non-empty sentences\")\n",
        "#     model = FastText(vector_size=ft_conf[\"vector_size\"],\n",
        "#                      window=ft_conf[\"window\"],\n",
        "#                      min_count=ft_conf[\"min_count\"],\n",
        "#                      workers=ft_conf[\"workers\"],\n",
        "#                      sg=ft_conf.get(\"sg\", 1))\n",
        "#     model.build_vocab(sentences=sentences)\n",
        "#     model.train(sentences=sentences, total_examples=len(sentences), epochs=ft_conf[\"epochs\"])\n",
        "#     model.save(path)\n",
        "#     return model\n",
        "def train_fasttext_gensim(sentences, save_path, cfg):\n",
        "    \"\"\"\n",
        "    Train a FastText model using gensim, ensuring the vector_size matches cfg.\n",
        "    sentences: list of token lists\n",
        "    save_path: path to save the model\n",
        "    cfg: configuration dictionary, e.g., {\"vector_size\":300, \"window\":5, \"min_count\":1, \"epochs\":5}\n",
        "    \"\"\"\n",
        "    vector_size = cfg.get(\"vector_size\", 300)  # default to 300 if not specified\n",
        "    window = cfg.get(\"window\", 5)\n",
        "    min_count = cfg.get(\"min_count\", 1)\n",
        "    epochs = cfg.get(\"epochs\", 5)\n",
        "    sg = cfg.get(\"sg\", 1)  # skip-gram by default\n",
        "\n",
        "    print(f\"Training FastText gensim model with vector_size={vector_size}...\")\n",
        "    ft_model = FastText(\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        sg=sg\n",
        "    )\n",
        "\n",
        "    ft_model.build_vocab(sentences)\n",
        "    ft_model.train(sentences, total_examples=len(sentences), epochs=epochs)\n",
        "\n",
        "    ft_model.save(save_path)\n",
        "    print(f\"Saved FastText gensim model to {save_path}\")\n",
        "    return ft_model\n",
        "\n",
        "def load_fasttext_gensim(path):\n",
        "    return FastText.load(path)\n",
        "\n",
        "def text_to_word_vectors(ft_model, tokens, max_len):\n",
        "    vsz = ft_model.vector_size\n",
        "    vecs = []\n",
        "    for t in tokens[:max_len]:\n",
        "        try:\n",
        "            v = ft_model.wv.get_vector(t)\n",
        "        except Exception:\n",
        "            v = np.zeros(vsz, dtype=np.float32)\n",
        "        vecs.append(v)\n",
        "    if len(vecs) < max_len:\n",
        "        vecs.extend([np.zeros(vsz, dtype=np.float32)] * (max_len - len(vecs)))\n",
        "    return np.array(vecs, dtype=np.float32)\n",
        "\n",
        "# -----------------------------\n",
        "# Model builders\n",
        "# -----------------------------\n",
        "def build_word_cnn(max_words, max_len, embedding_dim, n_classes, filters, num_filters, dropout):\n",
        "    inp = Input(shape=(max_len,), name=\"word_input\")\n",
        "    emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len, name=\"word_emb\")(inp)\n",
        "    convs = []\n",
        "    for f in filters:\n",
        "        c = Conv1D(filters=num_filters, kernel_size=f, activation=\"relu\")(emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    x = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_char_cnn(vocab_size, max_chars, embedding_dim, filters, num_filters, dropout, n_classes):\n",
        "    inp = Input(shape=(max_chars,), name=\"char_input\")\n",
        "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_chars, name=\"char_emb\")(inp)\n",
        "    convs = []\n",
        "    for f in filters:\n",
        "        c = Conv1D(filters=num_filters, kernel_size=f, activation=\"relu\")(emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    x = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_combined_cnn(word_conf, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_conf[\"max_len\"],), name=\"word_input\")\n",
        "    w_emb = Embedding(input_dim=word_conf[\"max_words\"], output_dim=word_conf[\"embedding_dim\"], input_length=word_conf[\"max_len\"])(w_in)\n",
        "    w_convs = []\n",
        "    for f in CFG[\"cnn\"][\"filters\"]:\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        w_convs.append(c)\n",
        "    w_feat = concatenate(w_convs) if len(w_convs) > 1 else w_convs[0]\n",
        "    w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_convs = []\n",
        "    for f in char_conf.get(\"filters\", [3,4,5]):\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        c_convs.append(c)\n",
        "    c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "    c_feat = Dropout(char_conf.get(\"dropout\", CFG[\"char\"].get(\"dropout\", 0.5)))(c_feat)\n",
        "\n",
        "    merged = concatenate([w_feat, c_feat])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_fasttext_combined_cnn(ft_embed_dim, word_max_len, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_max_len, ft_embed_dim), name=\"word_vec_input\")\n",
        "    convs = []\n",
        "    for f in CFG[\"cnn\"][\"filters\"]:\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_in)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    w_feat = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_convs = []\n",
        "    for f in char_conf.get(\"filters\", [3,4,5]):\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        c_convs.append(c)\n",
        "    c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "    c_feat = Dropout(char_conf.get(\"dropout\", CFG[\"char\"].get(\"dropout\", 0.5)))(c_feat)\n",
        "\n",
        "    merged = concatenate([w_feat, c_feat])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# RNN builders\n",
        "def build_word_rnn(max_words, max_len, embedding_dim, n_classes, rnn_units, dropout):\n",
        "    inp = Input(shape=(max_len,), name=\"word_input\")\n",
        "    emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len)(inp)\n",
        "    x = Bidirectional(LSTM(rnn_units))(emb)\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_char_rnn(vocab_size, max_chars, embedding_dim, rnn_units, dropout, n_classes):\n",
        "    inp = Input(shape=(max_chars,), name=\"char_input\")\n",
        "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_chars)(inp)\n",
        "    x = Bidirectional(LSTM(rnn_units))(emb)\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_combined_rnn(word_conf, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_conf[\"max_len\"],), name=\"word_input\")\n",
        "    w_emb = Embedding(input_dim=word_conf[\"max_words\"], output_dim=word_conf[\"embedding_dim\"], input_length=word_conf[\"max_len\"])(w_in)\n",
        "    w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_emb)\n",
        "    w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_x = Bidirectional(LSTM(char_conf.get(\"rnn_units\", CFG[\"rnn\"][\"rnn_units\"])))(c_emb)\n",
        "    c_x = Dropout(char_conf.get(\"dropout\", CFG[\"rnn\"][\"dropout\"]))(c_x)\n",
        "\n",
        "    merged = concatenate([w_x, c_x])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_fasttext_combined_rnn(ft_embed_dim, word_max_len, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_max_len, ft_embed_dim), name=\"word_vec_input\")\n",
        "    w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_in)\n",
        "    w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_x = Bidirectional(LSTM(char_conf.get(\"rnn_units\", CFG[\"rnn\"][\"rnn_units\"])))(c_emb)\n",
        "    c_x = Dropout(char_conf.get(\"dropout\", CFG[\"rnn\"][\"dropout\"]))(c_x)\n",
        "\n",
        "    merged = concatenate([w_x, c_x])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# Central training function\n",
        "# -----------------------------\n",
        "def train_model(model_name, csv_path=\"\", force_retrain=False):\n",
        "    model_name = str(model_name).strip()\n",
        "    if model_name not in MODEL_NAMES:\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "    append_log(model_name, f\"=== START TRAIN [{model_name}] ===\")\n",
        "    print(f\"Starting training for: {model_name}\")\n",
        "\n",
        "    paths = model_paths(model_name)\n",
        "    df = load_data(csv_path)\n",
        "    texts = df[\"trade_name\"].tolist()\n",
        "\n",
        "    # labels\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(df[\"reason\"])\n",
        "    classes = le.classes_\n",
        "    n_classes = len(classes)\n",
        "    np.save(paths[\"classes\"], classes, allow_pickle=True)\n",
        "    y_cat = to_categorical(y, num_classes=n_classes)\n",
        "\n",
        "    state = load_training_state(model_name) or {}\n",
        "    phases = state.get(\"phases\", {})\n",
        "    last_completed = int(phases.get(model_name, {}).get(\"last_completed_epoch\", 0))\n",
        "    initial_epoch = last_completed\n",
        "    epochs = CFG[\"training\"][\"epochs\"]\n",
        "\n",
        "    cb_epoch = ModelCheckpoint(paths[\"model_epoch_pattern\"], save_best_only=False, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
        "    cb_best = ModelCheckpoint(paths[\"model_best\"], save_best_only=True, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
        "    cb_early = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n",
        "    cb_state = EpochCheckpointCallback(model_name)\n",
        "\n",
        "    try:\n",
        "        # 1) cnn_word\n",
        "        if model_name == \"cnn_word\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                model = build_word_cnn(CFG[\"word\"][\"max_words\"], CFG[\"word\"][\"max_len\"], CFG[\"word\"][\"embedding_dim\"], n_classes, CFG[\"cnn\"][\"filters\"], CFG[\"cnn\"][\"num_filters\"], CFG[\"cnn\"][\"dropout\"])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_word, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 2) cnn_char\n",
        "        elif model_name == \"cnn_char\":\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                filters = CFG[\"char\"].get(\"filters\", [3,4,5])\n",
        "                model = build_char_cnn(vocab_size, CFG[\"char\"][\"max_chars\"], CFG[\"char\"][\"embedding_dim\"], filters, CFG[\"cnn\"][\"num_filters\"], CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"]), n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_char, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 3) cnn_combined\n",
        "        elif model_name == \"cnn_combined\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                word_conf = {\"max_len\": CFG[\"word\"][\"max_len\"], \"max_words\": CFG[\"word\"][\"max_words\"], \"embedding_dim\": CFG[\"word\"][\"embedding_dim\"]}\n",
        "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"filters\": CFG[\"char\"].get(\"filters\",[3,4,5]), \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])}\n",
        "                model = build_combined_cnn(word_conf, char_conf, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 4) cnn_fasttext_keras\n",
        "        elif model_name == \"cnn_fasttext_keras\":\n",
        "            # Keras-style fasttext: we use a trainable Embedding on word indices (no gensim)\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            vocab_size = min(CFG[\"word\"][\"max_words\"], (len(word_tok.word_index) + 1))\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            # char tokenizer for combined branch\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, _ = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                save_char_tokenizer(char_map, max(char_map.values())+1, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            # Model: word branch uses embedding with embedding_dim = fasttext_keras embedding dim\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                # Build CNN that uses trainable embedding for words (fasttext-like)\n",
        "                w_in = Input(shape=(CFG[\"word\"][\"max_len\"],), name=\"word_input\")\n",
        "                w_emb = Embedding(input_dim=vocab_size, output_dim=CFG[\"fasttext_keras\"][\"embedding_dim\"], input_length=CFG[\"word\"][\"max_len\"])(w_in)\n",
        "                convs = []\n",
        "                for f in CFG[\"cnn\"][\"filters\"]:\n",
        "                    c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_emb)\n",
        "                    c = GlobalMaxPooling1D()(c)\n",
        "                    convs.append(c)\n",
        "                w_feat = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "                w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "                # char branch\n",
        "                c_in = Input(shape=(CFG[\"char\"][\"max_chars\"],), name=\"char_input\")\n",
        "                c_emb = Embedding(input_dim=max(char_map.values())+1, output_dim=CFG[\"char\"][\"embedding_dim\"], input_length=CFG[\"char\"][\"max_chars\"])(c_in)\n",
        "                c_convs = []\n",
        "                for f in CFG[\"char\"].get(\"filters\",[3,4,5]):\n",
        "                    c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "                    c = GlobalMaxPooling1D()(c)\n",
        "                    c_convs.append(c)\n",
        "                c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "                c_feat = Dropout(CFG[\"char\"].get(\"dropout\", 0.5))(c_feat)\n",
        "\n",
        "                merged = concatenate([w_feat, c_feat])\n",
        "                merged = Dropout(0.5)(merged)\n",
        "                out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "                model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 5) cnn_fasttext_gensim\n",
        "        elif model_name == \"cnn_fasttext_gensim\":\n",
        "          ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
        "          sentences = [tokenize_for_fasttext(t) for t in texts]\n",
        "\n",
        "          # Load or train FastText gensim model\n",
        "          if os.path.exists(ft_path) and not force_retrain:\n",
        "              ft = load_fasttext_gensim(ft_path)\n",
        "          else:\n",
        "              ft = train_fasttext_gensim(sentences, ft_path, CFG[\"fasttext_gensim\"])\n",
        "\n",
        "          embed_dim = int(ft.vector_size)\n",
        "          max_len = CFG[\"word\"][\"max_len\"]\n",
        "\n",
        "          # Memory-efficient generator for word vectors\n",
        "          def word_vector_generator(texts, ft_model, max_len):\n",
        "              for t in texts:\n",
        "                  tokens = tokenize_for_fasttext(t)\n",
        "                  vecs = np.array([ft_model.wv[w] for w in tokens if w in ft_model.wv], dtype=np.float32)\n",
        "                  if vecs.shape[0] < max_len:\n",
        "                      pad = np.zeros((max_len - vecs.shape[0], embed_dim), dtype=np.float32)\n",
        "                      vecs = np.vstack([vecs, pad])\n",
        "                  else:\n",
        "                      vecs = vecs[:max_len]\n",
        "                  yield vecs\n",
        "\n",
        "          # Convert generator to array in batches to reduce memory usage\n",
        "          batch_size = 512  # adjust based on RAM\n",
        "          X_word = []\n",
        "          for start in range(0, len(texts), batch_size):\n",
        "              batch_vecs = np.stack(list(word_vector_generator(texts[start:start+batch_size], ft, max_len)), axis=0)\n",
        "              X_word.append(batch_vecs)\n",
        "          X_word = np.vstack(X_word)\n",
        "\n",
        "          # Character-level features\n",
        "          char_path = paths[\"char_tokenizer\"]\n",
        "          if os.path.exists(char_path) and not force_retrain:\n",
        "              char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "          else:\n",
        "              char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "              vocab_size = max(char_map.values()) + 1\n",
        "              save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "\n",
        "          X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "\n",
        "          # Load or build model\n",
        "          if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "              model = load_model(paths[\"model_best\"])\n",
        "          else:\n",
        "              char_conf = {\n",
        "                  \"max_chars\": CFG[\"char\"][\"max_chars\"],\n",
        "                  \"vocab_size\": vocab_size,\n",
        "                  \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"],\n",
        "                  \"filters\": CFG[\"char\"].get(\"filters\", [3,4,5]),\n",
        "                  \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])\n",
        "              }\n",
        "              model = build_fasttext_combined_cnn(embed_dim, CFG[\"word\"][\"max_len\"], char_conf, n_classes)\n",
        "\n",
        "          # Train with callbacks\n",
        "          if initial_epoch < epochs:\n",
        "              model.fit(\n",
        "                  [X_word, X_char], y_cat,\n",
        "                  batch_size=CFG[\"training\"][\"batch_size\"],\n",
        "                  epochs=epochs,\n",
        "                  initial_epoch=initial_epoch,\n",
        "                  validation_split=CFG[\"training\"][\"validation_split\"],\n",
        "                  callbacks=[cb_epoch, cb_best, cb_early, cb_state],\n",
        "                  verbose=1\n",
        "              )\n",
        "\n",
        "        # elif model_name == \"cnn_fasttext_gensim\":\n",
        "        #   print(\"\\n=== TRAINING: cnn_fasttext_gensim (memory-optimized) ===\")\n",
        "\n",
        "        #   ft_path = paths[\"fasttext_gensim\"]\n",
        "        #   n_samples = len(texts)\n",
        "\n",
        "        #   # --- 0) Local memory-friendly hyperparams (adjust if needed) ---\n",
        "        #   # You can override these in CFG if you want permanently.\n",
        "        #   max_len = min(CFG[\"word\"].get(\"max_len\", 25), 50)                # limit to 50 tokens max\n",
        "        #   embed_dim = min(CFG[\"fasttext_gensim\"].get(\"vector_size\", 300), 100)  # limit embedding dim to 100\n",
        "        #   batch_size = max(8, min(CFG[\"training\"][\"batch_size\"], 64))      # prefer small batch sizes\n",
        "        #   epochs = CFG[\"training\"][\"epochs\"]\n",
        "\n",
        "        #   # --------------------------------------------------------\n",
        "        #   # 1) Prepare tokenized sentences for FastText (cleaned)\n",
        "        #   # --------------------------------------------------------\n",
        "        #   sentences = [\n",
        "        #       [tok for tok in tokenize_for_fasttext(t) if tok and tok.strip()]\n",
        "        #       for t in texts\n",
        "        #   ]\n",
        "        #   if len(sentences) == 0:\n",
        "        #       raise ValueError(\"FastText gensim error: no valid sentences found.\")\n",
        "\n",
        "        #   # --------------------------------------------------------\n",
        "        #   # 2) Load or train FastText gensim (incremental, memory-friendly)\n",
        "        #   # --------------------------------------------------------\n",
        "        #   from gensim.models import FastText\n",
        "\n",
        "        #   ft = None\n",
        "        #   if os.path.exists(ft_path) and not force_retrain:\n",
        "        #       try:\n",
        "        #           ft = FastText.load(ft_path)\n",
        "        #           print(\"Loaded existing FastText model.\")\n",
        "        #       except Exception as e:\n",
        "        #           print(\"Failed to load existing FastText (will retrain). Error:\", e)\n",
        "        #           ft = None\n",
        "\n",
        "        #   if ft is None:\n",
        "        #       print(\"Training FastText (gensim) incrementally...\")\n",
        "        #       # Build FastText model with smaller vector size to save memory\n",
        "        #       ft = FastText(\n",
        "        #           vector_size=embed_dim,\n",
        "        #           window=CFG[\"fasttext_gensim\"].get(\"window\", 5),\n",
        "        #           min_count=CFG[\"fasttext_gensim\"].get(\"min_count\", 1),\n",
        "        #           sg=CFG[\"fasttext_gensim\"].get(\"sg\", 1),\n",
        "        #           workers=4,\n",
        "        #           seed=42\n",
        "        #       )\n",
        "        #       ft.build_vocab(corpus_iterable=sentences)\n",
        "        #       # Train incrementally epoch-by-epoch (avoid big internal memory peaks)\n",
        "        #       for ep in range(CFG[\"fasttext_gensim\"].get(\"epochs\", 5)):\n",
        "        #           ft.train(corpus_iterable=sentences, total_examples=len(sentences), epochs=1)\n",
        "        #           print(f\"  FastText epoch {ep+1} done\")\n",
        "        #       ft.save(ft_path)\n",
        "        #       print(\"Saved FastText gensim model:\", ft_path)\n",
        "\n",
        "        #   # Keep only keyed vectors (free other model state) to reduce RAM\n",
        "        #   try:\n",
        "        #       kv = ft.wv  # KeyedVectors view\n",
        "        #       # optionally delete ft model object to free memory if distinct\n",
        "        #       # del ft\n",
        "        #       ft = kv\n",
        "        #       print(\"Using ft.wv (KeyedVectors) to save memory.\")\n",
        "        #   except Exception:\n",
        "        #       # if ft is already KeyedVectors this may fail; keep ft as is\n",
        "        #       pass\n",
        "\n",
        "        #   # --------------------------------------------------------\n",
        "        #   # 3) Char tokenizer: ensure exists (we will compute per-sample)\n",
        "        #   # --------------------------------------------------------\n",
        "        #   char_path = paths[\"char_tokenizer\"]\n",
        "        #   if os.path.exists(char_path) and not force_retrain:\n",
        "        #       char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "        #       print(\"Loaded char tokenizer.\")\n",
        "        #   else:\n",
        "        #       print(\"Building char tokenizer...\")\n",
        "        #       char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "        #       vocab_size = max(char_map.values()) + 1\n",
        "        #       save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "        #       print(\"Saved char tokenizer.\")\n",
        "\n",
        "        #   char_max = CFG[\"char\"][\"max_chars\"]\n",
        "\n",
        "        #   # --------------------------------------------------------\n",
        "        #   # 4) On-the-fly converters (safe, fixed-shape)\n",
        "        #   # --------------------------------------------------------\n",
        "        #   def safe_vec(word):\n",
        "        #       \"\"\"Return vector or zeros; dtype float32.\"\"\"\n",
        "        #       try:\n",
        "        #           return ft[word]  # KeyedVectors lookup or FastText wv lookup\n",
        "        #       except Exception:\n",
        "        #           return np.zeros(embed_dim, dtype=np.float32)\n",
        "\n",
        "        #   def sentence_to_vecs_fixed(sentence):\n",
        "        #       \"\"\"Return (max_len, embed_dim) array for a sentence (never empty).\"\"\"\n",
        "        #       toks = tokenize_for_fasttext(sentence)\n",
        "        #       toks = [t.strip() for t in toks if t and t.strip()]\n",
        "        #       if len(toks) == 0:\n",
        "        #           return np.zeros((max_len, embed_dim), dtype=np.float32)\n",
        "        #       vecs = [safe_vec(t) for t in toks[:max_len]]\n",
        "        #       vecs = np.array(vecs, dtype=np.float32)\n",
        "        #       if vecs.shape[0] >= max_len:\n",
        "        #           return vecs[:max_len]\n",
        "        #       # pad\n",
        "        #       pad_len = max_len - vecs.shape[0]\n",
        "        #       pad = np.zeros((pad_len, embed_dim), dtype=np.float32)\n",
        "        #       return np.vstack([vecs, pad])\n",
        "\n",
        "        #   def char_seq_for_text(text):\n",
        "        #       \"\"\"Return (char_max,) int array for a single text.\"\"\"\n",
        "        #       # texts_to_char_sequences can take list – we reuse it for one element and pick the first\n",
        "        #       return texts_to_char_sequences([text], char_map, char_max)[0]\n",
        "\n",
        "        #   # --------------------------------------------------------\n",
        "        #   # 5) Build tf.data.Dataset generator to stream samples\n",
        "        #   # --------------------------------------------------------\n",
        "        #   import math, tensorflow as tf\n",
        "\n",
        "        #   def sample_generator():\n",
        "        #       for i, t in enumerate(texts):\n",
        "        #           x_word = sentence_to_vecs_fixed(t)                  # (max_len, embed_dim)\n",
        "        #           x_char = char_seq_for_text(t)                       # (char_max,)\n",
        "        #           y_vec = y_cat[i].astype(np.float32)                  # one-hot vector (n_classes,)\n",
        "        #           yield (x_word, x_char), y_vec\n",
        "\n",
        "        #   output_signature = (\n",
        "        #       (tf.TensorSpec(shape=(max_len, embed_dim), dtype=tf.float32),\n",
        "        #       tf.TensorSpec(shape=(char_max,), dtype=tf.int32)),\n",
        "        #       tf.TensorSpec(shape=(y_cat.shape[1],), dtype=tf.float32)\n",
        "        #   )\n",
        "\n",
        "        #   ds = tf.data.Dataset.from_generator(sample_generator, output_signature=output_signature)\n",
        "        #   # shuffle, batch, prefetch — tuned for memory\n",
        "        #   ds = ds.shuffle(buffer_size=4096).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        #   # Steps per epoch for Keras when using ds: optional (Keras infers if using dataset)\n",
        "        #   steps_per_epoch = math.ceil(n_samples / batch_size)\n",
        "\n",
        "        #   # --------------------------------------------------------\n",
        "        #   # 6) Build or load model (ensure it matches (max_len,embed_dim) input)\n",
        "        #   # --------------------------------------------------------\n",
        "        #   if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "        #       try:\n",
        "        #           model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "        #           print(\"Loaded existing model.\")\n",
        "        #       except Exception:\n",
        "        #           print(\"Failed to load existing model; rebuilding.\")\n",
        "        #           model = None\n",
        "        #   else:\n",
        "        #       model = None\n",
        "\n",
        "        #   if model is None:\n",
        "        #       # build_fasttext_combined_cnn(ft_embed_dim, word_max_len, char_conf, n_classes)\n",
        "        #       char_conf = {\"max_chars\": char_max, \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"],\n",
        "        #                   \"filters\": CFG[\"char\"].get(\"filters\", [3, 4, 5]), \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])}\n",
        "        #       model = build_fasttext_combined_cnn(embed_dim, max_len, char_conf, n_classes)\n",
        "        #       print(\"Built new model (fasttext + char).\")\n",
        "\n",
        "        #   # --------------------------------------------------------\n",
        "        #   # 7) Train with streamed dataset\n",
        "        #   # --------------------------------------------------------\n",
        "        #   callbacks = [cb_epoch, cb_best, cb_early, cb_state]\n",
        "        #   print(\"Starting training (streamed tf.data)...\")\n",
        "        #   if initial_epoch < epochs:\n",
        "        #       model.fit(ds, epochs=epochs, initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch, callbacks=callbacks, verbose=1)\n",
        "\n",
        "        #   # Save best model (ModelCheckpoint cb_best should already have saved best weights)\n",
        "        #   try:\n",
        "        #       model.save(paths[\"model_best\"])\n",
        "        #       print(\"Saved trained model:\", paths[\"model_best\"])\n",
        "        #   except Exception as e:\n",
        "        #       print(\"Warning: failed to save model via model.save()\", e)\n",
        "\n",
        "        #   # free large objects explicitly\n",
        "        #   del ds\n",
        "        #   del sentences\n",
        "        #   # keep ft as KeyedVectors in memory for later prediction if needed (small)\n",
        "        #   print(\"Finished training cnn_fasttext_gensim (memory-optimized).\")\n",
        "\n",
        "        # Memory Intecive\n",
        "        # elif model_name == \"cnn_fasttext_gensim\":\n",
        "        #     print(\"\\n=== Training: cnn_fasttext_gensim ===\")\n",
        "\n",
        "        #     ft_path = paths[\"fasttext_gensim\"]\n",
        "\n",
        "        #     # --------------------------------------------------------\n",
        "        #     # 1) PREPARE TOKENIZED SENTENCES FOR FASTTEXT\n",
        "        #     # --------------------------------------------------------\n",
        "        #     sentences = [\n",
        "        #         [tok for tok in tokenize_for_fasttext(t) if tok and tok.strip()]\n",
        "        #         for t in texts\n",
        "        #     ]\n",
        "        #     if len(sentences) == 0:\n",
        "        #         raise ValueError(\"FastText gensim error: No valid sentences found.\")\n",
        "\n",
        "        #     # --------------------------------------------------------\n",
        "        #     # 2) LOAD OR TRAIN FASTTEXT (GENSIM)\n",
        "        #     # --------------------------------------------------------\n",
        "        #     from gensim.models import FastText\n",
        "\n",
        "        #     ft = None\n",
        "        #     if os.path.exists(ft_path) and not force_retrain:\n",
        "        #         try:\n",
        "        #             ft = FastText.load(ft_path)\n",
        "        #             print(\"Loaded existing gensim FastText.\")\n",
        "        #         except Exception as e:\n",
        "        #             print(\"⚠ Failed to load FastText, retraining. Error:\", e)\n",
        "\n",
        "        #     if ft is None:\n",
        "        #         print(\"Training FastText (gensim)...\")\n",
        "        #         ft = FastText(\n",
        "        #             vector_size=CFG[\"fasttext_gensim\"][\"vector_size\"],\n",
        "        #             window=CFG[\"fasttext_gensim\"][\"window\"],\n",
        "        #             min_count=CFG[\"fasttext_gensim\"][\"min_count\"],\n",
        "        #             sg=1,\n",
        "        #             workers=4,\n",
        "        #             seed=42\n",
        "        #         )\n",
        "        #         ft.build_vocab(sentences)\n",
        "        #         ft.train(\n",
        "        #             corpus_iterable=sentences,\n",
        "        #             total_examples=len(sentences),\n",
        "        #             epochs=CFG[\"fasttext_gensim\"][\"epochs\"]\n",
        "        #         )\n",
        "        #         ft.save(ft_path)\n",
        "        #         print(\"Saved FastText gensim model:\", ft_path)\n",
        "\n",
        "        #     embed_dim = ft.vector_size\n",
        "        #     max_len = CFG[\"word\"][\"max_len\"]\n",
        "\n",
        "        #     # --------------------------------------------------------\n",
        "        #     # 3) SAFE WORD VECTOR EXTRACTION (NO EMPTY ARRAYS)\n",
        "        #     # --------------------------------------------------------\n",
        "        #     def safe_word_vec(word):\n",
        "        #         \"\"\"Return FT vector or zero vector.\"\"\"\n",
        "        #         return ft.wv[word] if word in ft.wv else np.zeros(embed_dim, dtype=np.float32)\n",
        "\n",
        "        #     def sentence_to_vecs(sentence, max_len):\n",
        "        #         \"\"\"Convert a text sentence → fixed (max_len, embed_dim) matrix safely.\"\"\"\n",
        "        #         tokens = tokenize_for_fasttext(sentence)\n",
        "        #         tokens = [t.strip() for t in tokens if t and t.strip()]\n",
        "\n",
        "        #         # If no tokens — return full zero representation\n",
        "        #         if len(tokens) == 0:\n",
        "        #             return np.zeros((max_len, embed_dim), dtype=np.float32)\n",
        "\n",
        "        #         vecs = np.array([safe_word_vec(tok) for tok in tokens], dtype=np.float32)\n",
        "\n",
        "        #         # TRUNCATE\n",
        "        #         if vecs.shape[0] >= max_len:\n",
        "        #             return vecs[:max_len]\n",
        "\n",
        "        #         # PAD\n",
        "        #         pad_len = max_len - vecs.shape[0]\n",
        "        #         pad = np.zeros((pad_len, embed_dim), dtype=np.float32)\n",
        "        #         return np.vstack([vecs, pad])\n",
        "\n",
        "        #     # Final model input: ALWAYS shape (N, max_len, embed_dim)\n",
        "        #     X_word = np.stack([sentence_to_vecs(t, max_len) for t in texts], axis=0)\n",
        "\n",
        "        #     # --------------------------------------------------------\n",
        "        #     # 4) CHAR TOKENIZER — LOAD OR CREATE\n",
        "        #     # --------------------------------------------------------\n",
        "        #     char_path = paths[\"char_tokenizer\"]\n",
        "\n",
        "        #     if os.path.exists(char_path) and not force_retrain:\n",
        "        #         char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "        #         print(\"Loaded char tokenizer.\")\n",
        "        #     else:\n",
        "        #         print(\"Building char tokenizer...\")\n",
        "        #         char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "        #         vocab_size = max(char_map.values()) + 1\n",
        "        #         save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "        #         print(\"Saved char tokenizer.\")\n",
        "\n",
        "        #     X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "\n",
        "        #     # --------------------------------------------------------\n",
        "        #     # 5) LOAD OR BUILD CNN MODEL\n",
        "        #     # --------------------------------------------------------\n",
        "        #     if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "        #         model = load_model(paths[\"model_best\"])\n",
        "        #         print(\"Loaded existing best model.\")\n",
        "        #     else:\n",
        "        #         char_conf = {\n",
        "        #             \"max_chars\": CFG[\"char\"][\"max_chars\"],\n",
        "        #             \"vocab_size\": vocab_size,\n",
        "        #             \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"],\n",
        "        #             \"filters\": CFG[\"char\"].get(\"filters\", [3, 4, 5]),\n",
        "        #             \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])\n",
        "        #         }\n",
        "\n",
        "        #         print(\"Building CNN FastText Gensim model...\")\n",
        "        #         model = build_fasttext_combined_cnn(\n",
        "        #             embed_dim=embed_dim,\n",
        "        #             max_len=max_len,\n",
        "        #             char_conf=char_conf,\n",
        "        #             num_classes=n_classes\n",
        "        #         )\n",
        "\n",
        "\n",
        "            # # --------------------------------------------------------\n",
        "            # # 6) TRAIN MODEL\n",
        "            # # --------------------------------------------------------\n",
        "            # print(\"Training model...\")\n",
        "\n",
        "            # if initial_epoch < epochs:\n",
        "            #     model.fit(\n",
        "            #         [X_word, X_char],\n",
        "            #         y_cat,\n",
        "            #         batch_size=CFG[\"training\"][\"batch_size\"],\n",
        "            #         epochs=epochs,\n",
        "            #         initial_epoch=initial_epoch,\n",
        "            #         validation_split=CFG[\"training\"][\"validation_split\"],\n",
        "            #         callbacks=[cb_epoch, cb_best, cb_early, cb_state],\n",
        "            #         verbose=1\n",
        "            #     )\n",
        "        # Memory Intecive\n",
        "\n",
        "        # 6) rnn_word\n",
        "        elif model_name == \"rnn_word\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                model = build_word_rnn(CFG[\"word\"][\"max_words\"], CFG[\"word\"][\"max_len\"], CFG[\"word\"][\"embedding_dim\"], n_classes, CFG[\"rnn\"][\"rnn_units\"], CFG[\"rnn\"][\"dropout\"])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_word, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 7) rnn_char\n",
        "        elif model_name == \"rnn_char\":\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                model = build_char_rnn(vocab_size, CFG[\"char\"][\"max_chars\"], CFG[\"char\"][\"embedding_dim\"], CFG[\"rnn\"][\"rnn_units\"], CFG[\"rnn\"][\"dropout\"], n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_char, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 8) rnn_combined\n",
        "        elif model_name == \"rnn_combined\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                word_conf = {\"max_len\": CFG[\"word\"][\"max_len\"], \"max_words\": CFG[\"word\"][\"max_words\"], \"embedding_dim\": CFG[\"word\"][\"embedding_dim\"]}\n",
        "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"rnn_units\": CFG[\"rnn\"][\"rnn_units\"], \"dropout\": CFG[\"rnn\"][\"dropout\"]}\n",
        "                model = build_combined_rnn(word_conf, char_conf, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 9) rnn_fasttext_keras\n",
        "        elif model_name == \"rnn_fasttext_keras\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            vocab_size = min(CFG[\"word\"][\"max_words\"], (len(word_tok.word_index) + 1))\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, _ = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                save_char_tokenizer(char_map, max(char_map.values())+1, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                # RNN combined with trainable embedding for words\n",
        "                w_in = Input(shape=(CFG[\"word\"][\"max_len\"],), name=\"word_input\")\n",
        "                w_emb = Embedding(input_dim=vocab_size, output_dim=CFG[\"fasttext_keras\"][\"embedding_dim\"], input_length=CFG[\"word\"][\"max_len\"])(w_in)\n",
        "                w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_emb)\n",
        "                w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "                c_in = Input(shape=(CFG[\"char\"][\"max_chars\"],), name=\"char_input\")\n",
        "                c_emb = Embedding(input_dim=max(char_map.values())+1, output_dim=CFG[\"char\"][\"embedding_dim\"], input_length=CFG[\"char\"][\"max_chars\"])(c_in)\n",
        "                c_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(c_emb)\n",
        "                c_x = Dropout(CFG[\"rnn\"][\"dropout\"])(c_x)\n",
        "\n",
        "                merged = concatenate([w_x, c_x])\n",
        "                merged = Dropout(0.5)(merged)\n",
        "                out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "                model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 10) rnn_fasttext_gensim\n",
        "        elif model_name == \"rnn_fasttext_gensim\":\n",
        "            ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
        "            sentences = [tokenize_for_fasttext(t) for t in texts]\n",
        "            if os.path.exists(ft_path) and not force_retrain:\n",
        "                ft = load_fasttext_gensim(ft_path)\n",
        "            else:\n",
        "                ft = train_fasttext_gensim(sentences, ft_path, CFG[\"fasttext_gensim\"])\n",
        "            embed_dim = ft.vector_size\n",
        "            max_len = CFG[\"word\"][\"max_len\"]\n",
        "            X_word = np.stack([text_to_word_vectors(ft, tokenize_for_fasttext(t), max_len) for t in texts], axis=0)\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                model = build_fasttext_combined_rnn(embed_dim, CFG[\"word\"][\"max_len\"], {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"rnn_units\": CFG[\"rnn\"][\"rnn_units\"], \"dropout\": CFG[\"rnn\"][\"dropout\"]}, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unhandled model: \" + model_name)\n",
        "\n",
        "    except Exception as exc:\n",
        "        tb = traceback.format_exc()\n",
        "        append_log(model_name, f\"TRAIN ERROR: {str(exc)}\\n{tb}\")\n",
        "        print(\"Error training\", model_name, \":\", str(exc))\n",
        "        raise\n",
        "\n",
        "    append_log(model_name, f\"=== FINISHED TRAIN [{model_name}] ===\")\n",
        "    print(f\"Training finished for: {model_name}\")\n",
        "    return True\n",
        "\n",
        "# -----------------------------\n",
        "# Resources & prediction\n",
        "# -----------------------------\n",
        "def load_resources_for_model(model_name):\n",
        "    if model_name not in MODEL_NAMES:\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "    paths = model_paths(model_name)\n",
        "    if not os.path.exists(paths[\"classes\"]):\n",
        "        raise FileNotFoundError(\"Classes file missing. Train first.\")\n",
        "    classes = np.load(paths[\"classes\"], allow_pickle=True)\n",
        "    if not os.path.exists(paths[\"model_best\"]):\n",
        "        raise FileNotFoundError(\"Best model missing. Train first.\")\n",
        "    model = load_model(paths[\"model_best\"])\n",
        "    res = {\"classes\": classes, \"model\": model}\n",
        "\n",
        "    if model_name in [\"cnn_word\", \"rnn_word\", \"cnn_combined\", \"rnn_combined\", \"cnn_fasttext_keras\", \"rnn_fasttext_keras\"]:\n",
        "        if not os.path.exists(paths[\"word_tokenizer\"]):\n",
        "            raise FileNotFoundError(\"Word tokenizer missing.\")\n",
        "        res[\"word_tokenizer\"] = load_tokenizer_json(paths[\"word_tokenizer\"])\n",
        "\n",
        "    if model_name in [\"cnn_char\", \"rnn_char\", \"cnn_combined\", \"rnn_combined\", \"cnn_fasttext_keras\", \"cnn_fasttext_gensim\", \"rnn_fasttext_keras\", \"rnn_fasttext_gensim\"]:\n",
        "        if not os.path.exists(paths[\"char_tokenizer\"]):\n",
        "            raise FileNotFoundError(\"Char tokenizer missing.\")\n",
        "        char_map, _ = load_char_tokenizer(paths[\"char_tokenizer\"])\n",
        "        res[\"char_map\"] = char_map\n",
        "\n",
        "    if model_name in [\"cnn_fasttext_gensim\", \"rnn_fasttext_gensim\", \"cnn_fasttext_gensim\"]:\n",
        "        ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
        "        if not os.path.exists(ft_path):\n",
        "            raise FileNotFoundError(\"FastText gensim model missing.\")\n",
        "        res[\"fasttext\"] = load_fasttext_gensim(ft_path)\n",
        "\n",
        "    return res\n",
        "\n",
        "def predict_for_model(model_name, text, resources):\n",
        "    t = simple_clean_text(text)\n",
        "    model = resources[\"model\"]\n",
        "    classes = resources[\"classes\"]\n",
        "    if model_name in [\"cnn_word\", \"rnn_word\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        x = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        preds = model.predict(x, verbose=0)\n",
        "    elif model_name in [\"cnn_char\", \"rnn_char\"]:\n",
        "        cm = resources[\"char_map\"]\n",
        "        x = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict(x, verbose=0)\n",
        "    elif model_name in [\"cnn_combined\", \"rnn_combined\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        xw = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    elif model_name in [\"cnn_fasttext_keras\", \"rnn_fasttext_keras\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        xw = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    elif model_name in [\"cnn_fasttext_gensim\", \"rnn_fasttext_gensim\"]:\n",
        "        ft = resources[\"fasttext\"]\n",
        "        tokens = tokenize_for_fasttext(t)\n",
        "        xw = np.expand_dims(text_to_word_vectors(ft, tokens, CFG[\"word\"][\"max_len\"]), axis=0)\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "\n",
        "    idx = int(np.argmax(preds, axis=1)[0])\n",
        "    prob = float(np.max(preds))\n",
        "    label = str(classes[idx])\n",
        "\n",
        "    # Clean the predicted label for comparison by removing Arabic semicolon and stripping whitespace\n",
        "    cleaned_label_for_comparison = label.replace('؛', '').strip()\n",
        "\n",
        "    # Check for both possible spellings of 'normal' based on user's input for acceptance\n",
        "    accepted_forms = [\"ኖርማል\", \"ኖርማል\"]\n",
        "    accepted = cleaned_label_for_comparison in accepted_forms\n",
        "\n",
        "    return {\"label\": label, \"probability\": prob, \"accepted\": accepted}\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI\n",
        "# -----------------------------\n",
        "def build_gradio():\n",
        "    def on_train(model_name, csv_path, force):\n",
        "        try:\n",
        "            train_model(model_name, csv_path=csv_path.strip() if csv_path else \"\", force_retrain=force)\n",
        "            return f\"Training finished for: {model_name}\"\n",
        "        except Exception as e:\n",
        "            return \"ERROR: \" + str(e)\n",
        "\n",
        "    def on_predict(model_name, name):\n",
        "        try:\n",
        "            res = load_resources_for_model(model_name)\n",
        "            r = predict_for_model(model_name, name, res)\n",
        "            label = r[\"label\"]\n",
        "            conf = f\"{r['probability']*100:.2f}%\"\n",
        "            st = \"ACCEPTED ✅\" if r[\"accepted\"] else \"REJECTED ❌\"\n",
        "            return label, conf, st\n",
        "        except Exception as e:\n",
        "            return \"ERROR: \" + str(e), \"\", \"\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Unified Models — select a model, train/resume, or predict\")\n",
        "        with gr.Row():\n",
        "            model_select = gr.Dropdown(MODEL_NAMES, value=MODEL_NAMES[0], label=\"Model\")\n",
        "            csv_input = gr.Textbox(label=\"CSV path (leave empty to use fallback)\", value=\"\")\n",
        "        with gr.Row():\n",
        "            train_btn = gr.Button(\"Train / Resume Selected Model\")\n",
        "            force_chk = gr.Checkbox(label=\"Force rebuild (delete/load fresh)\", value=False)\n",
        "            status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "        with gr.Row():\n",
        "            name_input = gr.Textbox(label=\"Proposed Trade Name\")\n",
        "            predict_btn = gr.Button(\"Predict\")\n",
        "        with gr.Row():\n",
        "            out_label = gr.Textbox(label=\"Predicted Reason\")\n",
        "            out_conf = gr.Textbox(label=\"Confidence\")\n",
        "            out_status = gr.Textbox(label=\"Decision\")\n",
        "\n",
        "        train_btn.click(on_train, inputs=[model_select, csv_input, force_chk], outputs=[status])\n",
        "        predict_btn.click(on_predict, inputs=[model_select, name_input], outputs=[out_label, out_conf, out_status])\n",
        "\n",
        "    return demo\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--csv', default='', help='Path to CSV dataset (optional)')\n",
        "    parser.add_argument('--train', type=str, help='Train a specific model (name)')\n",
        "    parser.add_argument('--force', action='store_true', help='Force rebuild')\n",
        "    parser.add_argument('--serve', action='store_true', help='Launch Gradio UI')\n",
        "    args = parser.parse_known_args()[0]\n",
        "\n",
        "    if args.train:\n",
        "        print(\"Training:\", args.train)\n",
        "        train_model(args.train, csv_path=args.csv, force_retrain=args.force)\n",
        "    elif args.serve:\n",
        "        demo = build_gradio()\n",
        "        demo.launch()\n",
        "    else:\n",
        "        print(\"Script ready. Use --train <model_name> or --serve to launch the UI.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQfD7Vvv3P3",
        "outputId": "3d176856-69f3-4f36-9e77-1000dce57cd0"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "import datetime\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Install gensim if not already present\n",
        "!pip install gensim\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
        "                                     Dense, Dropout, concatenate, LSTM, Bidirectional)\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# gensim FastText (real)\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Gradio\n",
        "import gradio as gr\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration (user-specified)\n",
        "# -----------------------------\n",
        "ROOT_MODEL_DIR = \"/content/drive/MyDrive/10models_aio_back\"\n",
        "CSV_FALLBACK = \"/content/drive/MyDrive/output_8_1_M_1_balanced_utf8.csv\"\n",
        "\n",
        "# Paths for FastText storage (user-specified)\n",
        "FT_GENSIM_DIR = \"/content/drive/MyDrive/10models_aio/fasttest_gensim_back\"\n",
        "FT_KERAS_DIR = \"/content/drive/MyDrive/10models_aio/fasttest_keras_back\"\n",
        "\n",
        "MODEL_NAMES = [\n",
        "    \"cnn_word\",\n",
        "    \"cnn_char\",\n",
        "    \"cnn_combined\",\n",
        "    \"cnn_fasttext_keras\",\n",
        "    \"cnn_fasttext_gensim\",\n",
        "    \"rnn_word\",\n",
        "    \"rnn_char\",\n",
        "    \"rnn_combined\",\n",
        "    \"rnn_fasttext_keras\",\n",
        "    \"rnn_fasttext_gensim\"\n",
        "]\n",
        "\n",
        "CFG = {\n",
        "    \"word\": {\"max_words\": 20000, \"max_len\": 25, \"embedding_dim\": 100},\n",
        "    \"char\": {\"max_chars\": 200, \"vocab_size\": 200, \"embedding_dim\": 64},\n",
        "    \"cnn\": {\"filters\": [2, 3, 4], \"num_filters\": 128, \"dropout\": 0.5},\n",
        "    \"rnn\": {\"rnn_units\": 128, \"dropout\": 0.5},\n",
        "    \"training\": {\"batch_size\": 64, \"epochs\": 2, \"validation_split\": 0.15},\n",
        "    # gensim FastText (real)\n",
        "    \"fasttext_gensim\": {\"vector_size\": 300, \"window\": 5, \"min_count\": 1, \"workers\": 4, \"epochs\": 10, \"sg\": 1},\n",
        "    # Keras-style fasttext embedding (trainable embedding layer, optional subword-like handling could be added)\n",
        "    \"fasttext_keras\": {\"embedding_dim\": 100}\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Ensure directories\n",
        "# -----------------------------\n",
        "def ensure_dirs():\n",
        "    os.makedirs(ROOT_MODEL_DIR, exist_ok=True)\n",
        "    os.makedirs(FT_GENSIM_DIR, exist_ok=True)\n",
        "    os.makedirs(FT_KERAS_DIR, exist_ok=True)\n",
        "    for mn in MODEL_NAMES:\n",
        "        os.makedirs(os.path.join(ROOT_MODEL_DIR, mn), exist_ok=True)\n",
        "\n",
        "ensure_dirs()\n",
        "\n",
        "# -----------------------------\n",
        "# Paths & helpers\n",
        "# -----------------------------\n",
        "def model_folder(model_name):\n",
        "    p = os.path.join(ROOT_MODEL_DIR, model_name)\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def model_paths(model_name):\n",
        "    base = model_folder(model_name)\n",
        "    return {\n",
        "        \"base\": base,\n",
        "        \"model_best\": os.path.join(base, \"model_best.h5\"),\n",
        "        \"model_epoch_pattern\": os.path.join(base, \"model_epoch-{epoch:02d}-val_loss-{val_loss:.4f}.h5\"),\n",
        "        \"word_tokenizer\": os.path.join(base, \"word_tokenizer.json\"),\n",
        "        \"char_tokenizer\": os.path.join(base, \"char_tokenizer.json\"),\n",
        "        \"classes\": os.path.join(base, \"classes.npy\"),\n",
        "        \"fasttext_gensim\": os.path.join(FT_GENSIM_DIR, f\"{model_name}_fasttext.model\"),\n",
        "        \"fasttext_keras\": os.path.join(FT_KERAS_DIR, f\"{model_name}_ft_keras.json\"),\n",
        "        \"training_state\": os.path.join(base, \"training_state.json\"),\n",
        "        \"train_log\": os.path.join(base, \"train.log\")\n",
        "    }\n",
        "\n",
        "def save_json_atomic(obj, path):\n",
        "    tmp = path + \".tmp\"\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def load_json(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def append_log(model_name, msg):\n",
        "    p = model_paths(model_name)[\"train_log\"]\n",
        "    ts = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
        "    with open(p, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"[{ts}] {msg}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Training state (resume)\n",
        "# -----------------------------\n",
        "def load_training_state(model_name):\n",
        "    p = model_paths(model_name)[\"training_state\"]\n",
        "    if os.path.exists(p):\n",
        "        try:\n",
        "            return load_json(p)\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def save_training_state(model_name, state):\n",
        "    p = model_paths(model_name)[\"training_state\"]\n",
        "    save_json_atomic(state, p)\n",
        "\n",
        "class EpochCheckpointCallback(Callback):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        state = load_training_state(self.model_name) or {}\n",
        "        if \"phases\" not in state:\n",
        "            state[\"phases\"] = {}\n",
        "        state[\"phases\"][self.model_name] = {\n",
        "            \"last_completed_epoch\": int(epoch) + 1,\n",
        "            \"updated_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"logs\": (logs or {})\n",
        "        }\n",
        "        save_training_state(self.model_name, state)\n",
        "\n",
        "# -----------------------------\n",
        "# Data loading & preprocessing\n",
        "# -----------------------------\n",
        "def simple_clean_text(text):\n",
        "    text = str(text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def load_data(csv_path=\"\"):\n",
        "    if csv_path and os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "    elif os.path.exists(CSV_FALLBACK):\n",
        "        df = pd.read_csv(CSV_FALLBACK)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"CSV not found at {csv_path} or fallback {CSV_FALLBACK}\")\n",
        "\n",
        "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
        "    if \"trade_name\" not in df.columns or \"reason\" not in df.columns:\n",
        "        raise ValueError(\"CSV must contain 'trade_name' and 'reason' columns\")\n",
        "    df = df[[\"trade_name\", \"reason\"]].dropna()\n",
        "    df[\"trade_name\"] = df[\"trade_name\"].astype(str).apply(simple_clean_text)\n",
        "    df[\"reason\"] = df[\"reason\"].astype(str).str.strip()\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenizers & char mapping\n",
        "# -----------------------------\n",
        "def build_word_tokenizer(texts, max_words):\n",
        "    tok = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "    tok.fit_on_texts(texts)\n",
        "    return tok\n",
        "\n",
        "def save_tokenizer_json(tokenizer, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(tokenizer.to_json())\n",
        "\n",
        "def load_tokenizer_json(path):\n",
        "    from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return tokenizer_from_json(f.read())\n",
        "\n",
        "def build_char_tokenizer(texts, max_vocab=None):\n",
        "    chars = set()\n",
        "    for s in texts:\n",
        "        for ch in s:\n",
        "            chars.add(ch)\n",
        "    chars = sorted(chars)\n",
        "    if max_vocab is not None:\n",
        "        chars = chars[:max_vocab-2]\n",
        "    char_to_index = {ch: idx+2 for idx, ch in enumerate(chars)}\n",
        "    char_to_index[\"<PAD>\"] = 0\n",
        "    char_to_index[\"<OOV>\"] = 1\n",
        "    return char_to_index\n",
        "\n",
        "def save_char_tokenizer(char_map, vocab_size, path):\n",
        "    save_json_atomic({\"char_to_index\": char_map, \"vocab_size\": vocab_size}, path)\n",
        "\n",
        "def load_char_tokenizer(path):\n",
        "    data = load_json(path)\n",
        "    return data[\"char_to_index\"], data.get(\"vocab_size\", max(data[\"char_to_index\"].values()) + 1)\n",
        "\n",
        "def texts_to_char_sequences(texts, char_to_index, max_len):\n",
        "    seqs = []\n",
        "    pad = char_to_index.get(\"<PAD>\")\n",
        "    oov = char_to_index.get(\"<OOV>\")\n",
        "    for s in texts:\n",
        "        arr = [char_to_index.get(ch, oov) for ch in s]\n",
        "        if len(arr) < max_len:\n",
        "            arr = arr + [pad] * (max_len - len(arr))\n",
        "        else:\n",
        "            arr = arr[:max_len]\n",
        "        seqs.append(arr)\n",
        "    return np.array(seqs, dtype=np.int32)\n",
        "\n",
        "# -----------------------------\n",
        "# FastText (gensim) helpers\n",
        "# -----------------------------\n",
        "def tokenize_for_fasttext(text):\n",
        "    return simple_preprocess(text, deacc=False)\n",
        "\n",
        "# def train_fasttext_gensim(sentences, path, ft_conf):\n",
        "#     if not sentences or len(sentences) == 0:\n",
        "#         raise ValueError(\"FastText requires non-empty sentences\")\n",
        "#     model = FastText(vector_size=ft_conf[\"vector_size\"],\n",
        "#                      window=ft_conf[\"window\"],\n",
        "#                      min_count=ft_conf[\"min_count\"],\n",
        "#                      workers=ft_conf[\"workers\"],\n",
        "#                      sg=ft_conf.get(\"sg\", 1))\n",
        "#     model.build_vocab(sentences=sentences)\n",
        "#     model.train(sentences=sentences, total_examples=len(sentences), epochs=ft_conf[\"epochs\"])\n",
        "#     model.save(path)\n",
        "#     return model\n",
        "def train_fasttext_gensim(sentences, save_path, cfg):\n",
        "    \"\"\"\n",
        "    Train a FastText model using gensim, ensuring the vector_size matches cfg.\n",
        "    sentences: list of token lists\n",
        "    save_path: path to save the model\n",
        "    cfg: configuration dictionary, e.g., {\"vector_size\":300, \"window\":5, \"min_count\":1, \"epochs\":5}\n",
        "    \"\"\"\n",
        "    vector_size = cfg.get(\"vector_size\", 300)  # default to 300 if not specified\n",
        "    window = cfg.get(\"window\", 5)\n",
        "    min_count = cfg.get(\"min_count\", 1)\n",
        "    epochs = cfg.get(\"epochs\", 5)\n",
        "    sg = cfg.get(\"sg\", 1)  # skip-gram by default\n",
        "\n",
        "    print(f\"Training FastText gensim model with vector_size={vector_size}...\")\n",
        "    ft_model = FastText(\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        sg=sg\n",
        "    )\n",
        "\n",
        "    ft_model.build_vocab(sentences)\n",
        "    ft_model.train(sentences, total_examples=len(sentences), epochs=epochs)\n",
        "\n",
        "    ft_model.save(save_path)\n",
        "    print(f\"Saved FastText gensim model to {save_path}\")\n",
        "    return ft_model\n",
        "\n",
        "def load_fasttext_gensim(path):\n",
        "    return FastText.load(path)\n",
        "\n",
        "def text_to_word_vectors(ft_model, tokens, max_len):\n",
        "    vsz = ft_model.vector_size\n",
        "    vecs = []\n",
        "    for t in tokens[:max_len]:\n",
        "        v = np.zeros(vsz, dtype=np.float32) # Default to zero vector\n",
        "        try:\n",
        "            retrieved_v = ft_model.wv[t]\n",
        "            if retrieved_v.shape == (vsz,): # Ensure it has the expected shape\n",
        "                v = retrieved_v\n",
        "        except KeyError:\n",
        "            # Already initialized to zero vector if word not found\n",
        "            pass\n",
        "        vecs.append(v)\n",
        "    if len(vecs) < max_len:\n",
        "        vecs.extend([np.zeros(vsz, dtype=np.float32)] * (max_len - len(vecs)))\n",
        "    return np.array(vecs, dtype=np.float32)\n",
        "\n",
        "# -----------------------------\n",
        "# Model builders\n",
        "# -----------------------------\n",
        "def build_word_cnn(max_words, max_len, embedding_dim, n_classes, filters, num_filters, dropout):\n",
        "    inp = Input(shape=(max_len,), name=\"word_input\")\n",
        "    emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len, name=\"word_emb\")(inp)\n",
        "    convs = []\n",
        "    for f in filters:\n",
        "        c = Conv1D(filters=num_filters, kernel_size=f, activation=\"relu\")(emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    x = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_char_cnn(vocab_size, max_chars, embedding_dim, filters, num_filters, dropout, n_classes):\n",
        "    inp = Input(shape=(max_chars,), name=\"char_input\")\n",
        "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_chars, name=\"char_emb\")(inp)\n",
        "    convs = []\n",
        "    for f in filters:\n",
        "        c = Conv1D(filters=num_filters, kernel_size=f, activation=\"relu\")(emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    x = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_combined_cnn(word_conf, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_conf[\"max_len\"],), name=\"word_input\")\n",
        "    w_emb = Embedding(input_dim=word_conf[\"max_words\"], output_dim=word_conf[\"embedding_dim\"], input_length=word_conf[\"max_len\"])(w_in)\n",
        "    w_convs = []\n",
        "    for f in CFG[\"cnn\"][\"filters\"]:\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        w_convs.append(c)\n",
        "    w_feat = concatenate(w_convs) if len(w_convs) > 1 else w_convs[0]\n",
        "    w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_convs = []\n",
        "    for f in char_conf.get(\"filters\", [3,4,5]):\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        c_convs.append(c)\n",
        "    c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "    c_feat = Dropout(char_conf.get(\"dropout\", CFG[\"char\"].get(\"dropout\", 0.5)))(c_feat)\n",
        "\n",
        "    merged = concatenate([w_feat, c_feat])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_fasttext_combined_cnn(ft_embed_dim, word_max_len, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_max_len, ft_embed_dim), name=\"word_vec_input\")\n",
        "    convs = []\n",
        "    for f in CFG[\"cnn\"][\"filters\"]:\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_in)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    w_feat = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_convs = []\n",
        "    for f in char_conf.get(\"filters\", [3,4,5]):\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        c_convs.append(c)\n",
        "    c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "    c_feat = Dropout(char_conf.get(\"dropout\", CFG[\"cnn\"][\"dropout\"]))(c_feat)\n",
        "\n",
        "    merged = concatenate([w_feat, c_feat])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# RNN builders\n",
        "def build_word_rnn(max_words, max_len, embedding_dim, n_classes, rnn_units, dropout):\n",
        "    inp = Input(shape=(max_len,), name=\"word_input\")\n",
        "    emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len)(inp)\n",
        "    x = Bidirectional(LSTM(rnn_units))(emb)\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_char_rnn(vocab_size, max_chars, embedding_dim, rnn_units, dropout, n_classes):\n",
        "    inp = Input(shape=(max_chars,), name=\"char_input\")\n",
        "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_chars)(inp)\n",
        "    x = Bidirectional(LSTM(rnn_units))(emb)\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_combined_rnn(word_conf, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_conf[\"max_len\"],), name=\"word_input\")\n",
        "    w_emb = Embedding(input_dim=word_conf[\"max_words\"], output_dim=word_conf[\"embedding_dim\"], input_length=word_conf[\"max_len\"])(w_in)\n",
        "    w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_emb)\n",
        "    w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_x = Bidirectional(LSTM(char_conf.get(\"rnn_units\", CFG[\"rnn\"][\"rnn_units\"]))) (c_emb)\n",
        "    c_x = Dropout(char_conf.get(\"dropout\", CFG[\"rnn\"][\"dropout\"]))(c_x)\n",
        "\n",
        "    merged = concatenate([w_x, c_x])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_fasttext_combined_rnn(ft_embed_dim, word_max_len, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_max_len, ft_embed_dim), name=\"word_vec_input\")\n",
        "    w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"])) (w_in)\n",
        "    w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_x = Bidirectional(LSTM(char_conf.get(\"rnn_units\", CFG[\"rnn\"][\"rnn_units\"]))) (c_emb)\n",
        "    c_x = Dropout(char_conf.get(\"dropout\", CFG[\"rnn\"][\"dropout\"]))(c_x)\n",
        "\n",
        "    merged = concatenate([w_x, c_x])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# Central training function\n",
        "# -----------------------------\n",
        "def train_model(model_name, csv_path=\"\", force_retrain=False):\n",
        "    model_name = str(model_name).strip()\n",
        "    if model_name not in MODEL_NAMES:\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "    append_log(model_name, f\"=== START TRAIN [{model_name}] ===\")\n",
        "    print(f\"Starting training for: {model_name}\")\n",
        "\n",
        "    paths = model_paths(model_name)\n",
        "    df = load_data(csv_path)\n",
        "    texts = df[\"trade_name\"].tolist()\n",
        "\n",
        "    # labels\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(df[\"reason\"])\n",
        "    classes = le.classes_\n",
        "    n_classes = len(classes)\n",
        "    np.save(paths[\"classes\"], classes, allow_pickle=True)\n",
        "    y_cat = to_categorical(y, num_classes=n_classes)\n",
        "\n",
        "    state = load_training_state(model_name) or {}\n",
        "    phases = state.get(\"phases\", {})\n",
        "    last_completed = int(phases.get(model_name, {}).get(\"last_completed_epoch\", 0))\n",
        "    initial_epoch = last_completed\n",
        "    epochs = CFG[\"training\"][\"epochs\"]\n",
        "\n",
        "    cb_epoch = ModelCheckpoint(paths[\"model_epoch_pattern\"], save_best_only=False, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
        "    cb_best = ModelCheckpoint(paths[\"model_best\"], save_best_only=True, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
        "    cb_early = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n",
        "    cb_state = EpochCheckpointCallback(model_name)\n",
        "\n",
        "    try:\n",
        "        # 1) cnn_word\n",
        "        if model_name == \"cnn_word\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                model = build_word_cnn(CFG[\"word\"][\"max_words\"], CFG[\"word\"][\"max_len\"], CFG[\"word\"][\"embedding_dim\"], n_classes, CFG[\"cnn\"][\"filters\"], CFG[\"cnn\"][\"num_filters\"], CFG[\"cnn\"][\"dropout\"])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_word, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 2) cnn_char\n",
        "        elif model_name == \"cnn_char\":\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                filters = CFG[\"char\"].get(\"filters\", [3,4,5])\n",
        "                model = build_char_cnn(vocab_size, CFG[\"char\"][\"max_chars\"], CFG[\"char\"][\"embedding_dim\"], filters, CFG[\"cnn\"][\"num_filters\"], CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"]), n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_char, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 3) cnn_combined\n",
        "        elif model_name == \"cnn_combined\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                word_conf = {\"max_len\": CFG[\"word\"][\"max_len\"], \"max_words\": CFG[\"word\"][\"max_words\"], \"embedding_dim\": CFG[\"word\"][\"embedding_dim\"]}\n",
        "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"filters\": CFG[\"char\"].get(\"filters\",[3,4,5]), \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])}\n",
        "                model = build_combined_cnn(word_conf, char_conf, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 4) cnn_fasttext_keras\n",
        "        elif model_name == \"cnn_fasttext_keras\":\n",
        "            # Keras-style fasttext: we use a trainable Embedding on word indices (no gensim)\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            vocab_size = min(CFG[\"word\"][\"max_words\"], (len(word_tok.word_index) + 1))\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            # char tokenizer for combined branch\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, _ = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                save_char_tokenizer(char_map, max(char_map.values())+1, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            # Model: word branch uses embedding with embedding_dim = fasttext_keras embedding dim\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                # Build CNN that uses trainable embedding for words (fasttext-like)\n",
        "                w_in = Input(shape=(CFG[\"word\"][\"max_len\"],), name=\"word_input\")\n",
        "                w_emb = Embedding(input_dim=vocab_size, output_dim=CFG[\"fasttext_keras\"][\"embedding_dim\"], input_length=CFG[\"word\"][\"max_len\"])(w_in)\n",
        "                convs = []\n",
        "                for f in CFG[\"cnn\"][\"filters\"]:\n",
        "                    c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_emb)\n",
        "                    c = GlobalMaxPooling1D()(c)\n",
        "                    convs.append(c)\n",
        "                w_feat = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "                w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "                # char branch\n",
        "                c_in = Input(shape=(CFG[\"char\"][\"max_chars\"],), name=\"char_input\")\n",
        "                c_emb = Embedding(input_dim=max(char_map.values())+1, output_dim=CFG[\"char\"][\"embedding_dim\"], input_length=CFG[\"char\"][\"max_chars\"])(c_in)\n",
        "                c_convs = []\n",
        "                for f in CFG[\"char\"].get(\"filters\",[3,4,5]):\n",
        "                    c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "                    c = GlobalMaxPooling1D()(c)\n",
        "                    c_convs.append(c)\n",
        "                c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "                c_feat = Dropout(CFG[\"char\"].get(\"dropout\", 0.5))(c_feat)\n",
        "\n",
        "                merged = concatenate([w_feat, c_feat])\n",
        "                merged = Dropout(0.5)(merged)\n",
        "                out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "                model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 5) cnn_fasttext_gensim\n",
        "        elif model_name == \"cnn_fasttext_gensim\":\n",
        "          ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
        "          sentences = [tokenize_for_fasttext(t) for t in texts]\n",
        "\n",
        "          # Load or train FastText gensim model\n",
        "          if os.path.exists(ft_path) and not force_retrain:\n",
        "              ft = load_fasttext_gensim(ft_path)\n",
        "          else:\n",
        "              ft = train_fasttext_gensim(sentences, ft_path, CFG[\"fasttext_gensim\"])\n",
        "\n",
        "          embed_dim = int(ft.vector_size)\n",
        "          max_len = CFG[\"word\"][\"max_len\"]\n",
        "\n",
        "          # Memory-efficient generator for word vectors\n",
        "          def word_vector_generator(texts, ft_model, max_len):\n",
        "              vsz = ft_model.vector_size\n",
        "              for t in texts:\n",
        "                  tokens = tokenize_for_fasttext(t)\n",
        "                  vec_list = []\n",
        "                  for w in tokens:\n",
        "                      v = np.zeros(vsz, dtype=np.float32) # Default to zero vector\n",
        "                      try:\n",
        "                          retrieved_v = ft_model.wv[w]\n",
        "                          if retrieved_v.shape == (vsz,): # Ensure it has the expected shape\n",
        "                              v = retrieved_v\n",
        "                      except KeyError:\n",
        "                          # Already initialized to zero vector if word not found\n",
        "                          pass\n",
        "                      vec_list.append(v)\n",
        "\n",
        "                  # Convert to numpy array. Explicitly handle empty case with correct 2nd dim\n",
        "                  if vec_list:\n",
        "                      vecs = np.array(vec_list, dtype=np.float32)\n",
        "                      # Apply padding/truncation only if vec_list was not empty\n",
        "                      if vecs.shape[0] < max_len:\n",
        "                          pad_len = max_len - vecs.shape[0]\n",
        "                          pad = np.zeros((pad_len, vsz), dtype=np.float32)\n",
        "                          vecs = np.vstack([vecs, pad])\n",
        "                      else:\n",
        "                          vecs = vecs[:max_len]\n",
        "                  else:\n",
        "                      # If vec_list is empty, means no valid tokens, return a full zero-padded array\n",
        "                      vecs = np.zeros((max_len, vsz), dtype=np.float32)\n",
        "                  yield vecs\n",
        "\n",
        "          # Character-level features\n",
        "          char_path = paths[\"char_tokenizer\"]\n",
        "          if os.path.exists(char_path) and not force_retrain:\n",
        "              char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "          else:\n",
        "              char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "              vocab_size = max(char_map.values()) + 1\n",
        "              save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "\n",
        "          X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "\n",
        "          # Split data for training and validation\n",
        "          val_split = CFG[\"training\"][\"validation_split\"]\n",
        "          dataset_size = len(texts)\n",
        "          val_size = int(dataset_size * val_split)\n",
        "          train_size = dataset_size - val_size\n",
        "\n",
        "          # Create a shuffled index to split data\n",
        "          indices = np.arange(dataset_size)\n",
        "          np.random.shuffle(indices)\n",
        "\n",
        "          train_indices = indices[:train_size]\n",
        "          val_indices = indices[train_size:]\n",
        "\n",
        "          train_texts = [texts[i] for i in train_indices]\n",
        "          train_X_char = X_char[train_indices]\n",
        "          train_y_cat = y_cat[train_indices]\n",
        "\n",
        "          val_texts = [texts[i] for i in val_indices]\n",
        "          val_X_char = X_char[val_indices]\n",
        "          val_y_cat = y_cat[val_indices]\n",
        "\n",
        "          # Create a TensorFlow Dataset for batching and GPU loading\n",
        "          train_dataset = tf.data.Dataset.from_generator(\n",
        "              lambda: (((w_vec, c_vec), label) for w_vec, c_vec, label in zip(word_vector_generator(train_texts, ft, max_len), train_X_char, train_y_cat)),\n",
        "              output_signature=(\n",
        "                  (tf.TensorSpec(shape=(max_len, embed_dim), dtype=tf.float32),\n",
        "                  tf.TensorSpec(shape=(CFG[\"char\"][\"max_chars\"],), dtype=tf.int32)),\n",
        "                  tf.TensorSpec(shape=(n_classes,), dtype=tf.float32)\n",
        "              )\n",
        "          ).batch(CFG[\"training\"][\"batch_size\"]).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "          val_dataset = tf.data.Dataset.from_generator(\n",
        "              lambda: (((w_vec, c_vec), label) for w_vec, c_vec, label in zip(word_vector_generator(val_texts, ft, max_len), val_X_char, val_y_cat)),\n",
        "              output_signature=(\n",
        "                  (tf.TensorSpec(shape=(max_len, embed_dim), dtype=tf.float32),\n",
        "                  tf.TensorSpec(shape=(CFG[\"char\"][\"max_chars\"],), dtype=tf.int32)),\n",
        "                  tf.TensorSpec(shape=(n_classes,), dtype=tf.float32)\n",
        "              )\n",
        "          ).batch(CFG[\"training\"][\"batch_size\"]).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "          # Load or build model\n",
        "          if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "              model = load_model(paths[\"model_best\"])\n",
        "          else:\n",
        "              char_conf = {\n",
        "                  \"max_chars\": CFG[\"char\"][\"max_chars\"],\n",
        "                  \"vocab_size\": vocab_size,\n",
        "                  \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"],\n",
        "                  \"filters\": CFG[\"char\"].get(\"filters\", [3,4,5]),\n",
        "                  \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])\n",
        "              }\n",
        "              model = build_fasttext_combined_cnn(embed_dim, max_len, char_conf, n_classes)\n",
        "\n",
        "          # Train with callbacks, using validation_data instead of validation_split\n",
        "          if initial_epoch < epochs:\n",
        "              model.fit(\n",
        "                  train_dataset,\n",
        "                  epochs=epochs,\n",
        "                  initial_epoch=initial_epoch,\n",
        "                  validation_data=val_dataset,\n",
        "                  callbacks=[cb_epoch, cb_best, cb_early, cb_state],\n",
        "                  verbose=1\n",
        "              )\n",
        "\n",
        "\n",
        "        # 6) rnn_word\n",
        "        elif model_name == \"rnn_word\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                model = build_word_rnn(CFG[\"word\"][\"max_words\"], CFG[\"word\"][\"max_len\"], CFG[\"word\"][\"embedding_dim\"], n_classes, CFG[\"rnn\"][\"rnn_units\"], CFG[\"rnn\"][\"dropout\"])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_word, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 7) rnn_char\n",
        "        elif model_name == \"rnn_char\":\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                filters = CFG[\"char\"].get(\"filters\", [3,4,5])\n",
        "                model = build_char_rnn(vocab_size, CFG[\"char\"][\"max_chars\"], CFG[\"char\"][\"embedding_dim\"], CFG[\"rnn\"][\"rnn_units\"], CFG[\"rnn\"][\"dropout\"], n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_char, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 8) rnn_combined\n",
        "        elif model_name == \"rnn_combined\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                word_conf = {\"max_len\": CFG[\"word\"][\"max_len\"], \"max_words\": CFG[\"word\"][\"max_words\"], \"embedding_dim\": CFG[\"word\"][\"embedding_dim\"]}\n",
        "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"rnn_units\": CFG[\"rnn\"][\"rnn_units\"], \"dropout\": CFG[\"rnn\"][\"dropout\"]}\n",
        "                model = build_combined_rnn(word_conf, char_conf, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 9) rnn_fasttext_keras\n",
        "        elif model_name == \"rnn_fasttext_keras\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            vocab_size = min(CFG[\"word\"][\"max_words\"], (len(word_tok.word_index) + 1))\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, _ = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                save_char_tokenizer(char_map, max(char_map.values())+1, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"])\n",
        "            else:\n",
        "                # RNN combined with trainable embedding for words\n",
        "                w_in = Input(shape=(CFG[\"word\"][\"max_len\"],), name=\"word_input\")\n",
        "                w_emb = Embedding(input_dim=vocab_size, output_dim=CFG[\"fasttext_keras\"][\"embedding_dim\"], input_length=CFG[\"word\"][\"max_len\"])(w_in)\n",
        "                w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"])) (w_emb)\n",
        "                w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "                c_in = Input(shape=(CFG[\"char\"][\"max_chars\"],), name=\"char_input\")\n",
        "                c_emb = Embedding(input_dim=max(char_map.values())+1, output_dim=CFG[\"char\"][\"embedding_dim\"], input_length=CFG[\"char\"][\"max_chars\"])(c_in)\n",
        "                c_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"])) (c_emb)\n",
        "                c_x = Dropout(CFG[\"rnn\"][\"dropout\"])(c_x)\n",
        "\n",
        "                merged = concatenate([w_x, c_x])\n",
        "                merged = Dropout(0.5)(merged)\n",
        "                out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "                model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=1)\n",
        "\n",
        "        # 10) rnn_fasttext_gensim\n",
        "        elif model_name == \"rnn_fasttext_gensim\":\n",
        "\n",
        "            # TPU strategy setup\n",
        "            try:\n",
        "                resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "                tf.config.experimental_connect_to_cluster(resolver)\n",
        "                tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "                strategy = tf.distribute.TPUStrategy(resolver)\n",
        "                print(\"Running on TPU:\", resolver.master())\n",
        "            except ValueError:\n",
        "                strategy = tf.distribute.get_strategy()\n",
        "                print(\"Running on default strategy (CPU/GPU)\")\n",
        "\n",
        "            def train_rnn_fasttext(texts, y_cat, paths, CFG, force_retrain=False, initial_epoch=0, epochs=2, n_classes=None):\n",
        "                try:\n",
        "                    model_name = \"rnn_fasttext_gensim\"\n",
        "\n",
        "                    # --- FastText embeddings ---\n",
        "                    ft_path = paths.get(\"fasttext_gensim\")\n",
        "                    sentences = [tokenize_for_fasttext(t) for t in texts]\n",
        "                    if os.path.exists(ft_path) and not force_retrain:\n",
        "                        ft = load_fasttext_gensim(ft_path)\n",
        "                    else:\n",
        "                        ft = train_fasttext_gensim(sentences, ft_path, CFG[\"fasttext_gensim\"])\n",
        "                    embed_dim = ft.vector_size\n",
        "                    max_len = CFG[\"word\"][\"max_len\"]\n",
        "                    X_word = np.stack([text_to_word_vectors(ft, tokenize_for_fasttext(t), max_len) for t in texts], axis=0)\n",
        "\n",
        "                    # --- Char embeddings ---\n",
        "                    char_path = paths[\"char_tokenizer\"]\n",
        "                    if os.path.exists(char_path) and not force_retrain:\n",
        "                        char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "                    else:\n",
        "                        char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                        vocab_size = max(char_map.values()) + 1\n",
        "                        save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "                    X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "\n",
        "                    # --- Model building ---\n",
        "                    model_best_path = paths[\"model_best\"]\n",
        "                    with strategy.scope():\n",
        "                        if os.path.exists(model_best_path) and not force_retrain:\n",
        "                            model = load_model(model_best_path)\n",
        "                        else:\n",
        "                            model = build_fasttext_combined_rnn(\n",
        "                                embed_dim,\n",
        "                                CFG[\"word\"][\"max_len\"],\n",
        "                                {\n",
        "                                    \"max_chars\": CFG[\"char\"][\"max_chars\"],\n",
        "                                    \"vocab_size\": vocab_size,\n",
        "                                    \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"],\n",
        "                                    \"rnn_units\": CFG[\"rnn\"][\"rnn_units\"],\n",
        "                                    \"dropout\": CFG[\"rnn\"][\"dropout\"]\n",
        "                                },\n",
        "                                n_classes\n",
        "                            )\n",
        "\n",
        "                    # --- Training ---\n",
        "                    if initial_epoch < epochs:\n",
        "                        model.fit(\n",
        "                            [X_word, X_char],\n",
        "                            y_cat,\n",
        "                            batch_size=CFG[\"training\"][\"batch_size\"],\n",
        "                            epochs=epochs,\n",
        "                            initial_epoch=initial_epoch,\n",
        "                            validation_split=CFG[\"training\"][\"validation_split\"],\n",
        "                            callbacks=[cb_epoch, cb_best, cb_early, cb_state],\n",
        "                            verbose=1\n",
        "                        )\n",
        "\n",
        "                except Exception as exc:\n",
        "                    tb = traceback.format_exc()\n",
        "                    append_log(model_name, f\"TRAIN ERROR: {str(exc)}\\n{tb}\")\n",
        "                    print(\"Error training\", model_name, \":\", str(exc))\n",
        "                    raise\n",
        "\n",
        "                append_log(model_name, f\"=== FINISHED TRAIN [{model_name}] ===\")\n",
        "                print(f\"Training finished for: {model_name}\")\n",
        "                return True\n",
        "\n",
        "            # --- IMPORTANT: CALL the function so the elif block finishes correctly ---\n",
        "            train_rnn_fasttext(texts, y_cat, paths, CFG, force_retrain, initial_epoch, epochs, n_classes)\n",
        "        # END OF ELIF BLOCK ✔️\n",
        "\n",
        "    except Exception as exc:\n",
        "        tb = traceback.format_exc()\n",
        "        append_log(model_name, f\"TRAIN ERROR: {str(exc)}\\n{tb}\")\n",
        "        print(\"Error training\", model_name, \":\", str(exc))\n",
        "        raise\n",
        "\n",
        "    append_log(model_name, f\"=== FINISHED TRAIN [{model_name}] ===\")\n",
        "    print(f\"Training finished for: {model_name}\")\n",
        "    return True\n",
        "\n",
        "# -----------------------------\n",
        "# Resources & prediction\n",
        "# -----------------------------\n",
        "def load_resources_for_model(model_name):\n",
        "    if model_name not in MODEL_NAMES:\n",
        "\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "    paths = model_paths(model_name)\n",
        "    if not os.path.exists(paths[\"classes\"]):\n",
        "        raise FileNotFoundError(\"Classes file missing. Train first.\")\n",
        "    classes = np.load(paths[\"classes\"], allow_pickle=True)\n",
        "    if not os.path.exists(paths[\"model_best\"]):\n",
        "        raise FileNotFoundError(\"Best model missing. Train first.\")\n",
        "    model = load_model(paths[\"model_best\"])\n",
        "    res = {\"classes\": classes, \"model\": model}\n",
        "\n",
        "    if model_name in [\"cnn_word\", \"rnn_word\", \"cnn_combined\", \"rnn_combined\", \"cnn_fasttext_keras\", \"rnn_fasttext_keras\"]:\n",
        "        if not os.path.exists(paths[\"word_tokenizer\"]):\n",
        "            raise FileNotFoundError(\"Word tokenizer missing.\")\n",
        "        res[\"word_tokenizer\"] = load_tokenizer_json(paths[\"word_tokenizer\"])\n",
        "\n",
        "    if model_name in [\"cnn_char\", \"rnn_char\", \"cnn_combined\", \"rnn_combined\", \"cnn_fasttext_keras\", \"cnn_fasttext_gensim\", \"rnn_fasttext_keras\", \"rnn_fasttext_gensim\"]:\n",
        "        if not os.path.exists(paths[\"char_tokenizer\"]):\n",
        "            raise FileNotFoundError(\"Char tokenizer missing.\")\n",
        "        char_map, _ = load_char_tokenizer(paths[\"char_tokenizer\"])\n",
        "        res[\"char_map\"] = char_map\n",
        "\n",
        "    if model_name in [\"cnn_fasttext_gensim\", \"rnn_fasttext_gensim\", \"cnn_fasttext_gensim\"]:\n",
        "        ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
        "        if not os.path.exists(ft_path):\n",
        "            raise FileNotFoundError(\"FastText gensim model missing.\")\n",
        "        res[\"fasttext\"] = load_fasttext_gensim(ft_path)\n",
        "\n",
        "    return res\n",
        "\n",
        "def predict_for_model(model_name, text, resources):\n",
        "    t = simple_clean_text(text)\n",
        "    model = resources[\"model\"]\n",
        "    classes = resources[\"classes\"]\n",
        "    if model_name in [\"cnn_word\", \"rnn_word\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        x = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        preds = model.predict(x, verbose=0)\n",
        "    elif model_name in [\"cnn_char\", \"rnn_char\"]:\n",
        "        cm = resources[\"char_map\"]\n",
        "        x = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict(x, verbose=0)\n",
        "    elif model_name in [\"cnn_combined\", \"rnn_combined\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        xw = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    elif model_name in [\"cnn_fasttext_keras\", \"rnn_fasttext_keras\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        xw = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    elif model_name in [\"cnn_fasttext_gensim\", \"rnn_fasttext_gensim\"]:\n",
        "        ft = resources[\"fasttext\"]\n",
        "        tokens = tokenize_for_fasttext(t)\n",
        "        xw = np.expand_dims(text_to_word_vectors(ft, tokens, CFG[\"word\"][\"max_len\"]), axis=0)\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "\n",
        "    idx = int(np.argmax(preds, axis=1)[0])\n",
        "    prob = float(np.max(preds))\n",
        "    label = str(classes[idx])\n",
        "\n",
        "    # Clean the predicted label for comparison by removing Arabic semicolon and stripping whitespace\n",
        "    cleaned_label_for_comparison = label.replace('؛', '').strip()\n",
        "\n",
        "    # Check for both possible spellings of 'normal' based on user's input for acceptance\n",
        "    accepted_forms = [\"ኖርማል\", \"ኖርማል\"]\n",
        "    accepted = cleaned_label_for_comparison in accepted_forms\n",
        "\n",
        "    return {\"label\": label, \"probability\": prob, \"accepted\": accepted}\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI\n",
        "# -----------------------------\n",
        "def build_gradio():\n",
        "    def on_train(model_name, csv_path, force):\n",
        "        try:\n",
        "            train_model(model_name, csv_path=csv_path.strip() if csv_path else \"\", force_retrain=force)\n",
        "            return f\"Training finished for: {model_name}\"\n",
        "        except Exception as e:\n",
        "            return \"ERROR: \" + str(e)\n",
        "\n",
        "    def on_predict(model_name, name):\n",
        "        try:\n",
        "            res = load_resources_for_model(model_name)\n",
        "            r = predict_for_model(model_name, name, res)\n",
        "            label = r[\"label\"]\n",
        "            conf = f\"{r['probability']*100:.2f}%\"\n",
        "            st = \"ACCEPTED ✅\" if r[\"accepted\"] else \"REJECTED ❌\"\n",
        "            return label, conf, st\n",
        "        except Exception as e:\n",
        "            return \"ERROR: \" + str(e), \"\", \"\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Unified Models — select a model, train/resume, or predict\")\n",
        "        with gr.Row():\n",
        "            model_select = gr.Dropdown(MODEL_NAMES, value=MODEL_NAMES[0], label=\"Model\")\n",
        "            csv_input = gr.Textbox(label=\"CSV path (leave empty to use fallback)\", value=\"\")\n",
        "        with gr.Row():\n",
        "            train_btn = gr.Button(\"Train / Resume Selected Model\")\n",
        "            force_chk = gr.Checkbox(label=\"Force rebuild (delete/load fresh)\", value=False)\n",
        "            status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "        with gr.Row():\n",
        "            name_input = gr.Textbox(label=\"Proposed Trade Name\")\n",
        "            predict_btn = gr.Button(\"Predict\")\n",
        "        with gr.Row():\n",
        "            out_label = gr.Textbox(label=\"Predicted Reason\")\n",
        "            out_conf = gr.Textbox(label=\"Confidence\")\n",
        "            out_status = gr.Textbox(label=\"Decision\")\n",
        "\n",
        "        train_btn.click(on_train, inputs=[model_select, csv_input, force_chk], outputs=[status])\n",
        "        predict_btn.click(on_predict, inputs=[model_select, name_input], outputs=[out_label, out_conf, out_status])\n",
        "\n",
        "    return demo\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--csv', default='', help='Path to CSV dataset (optional)')\n",
        "    parser.add_argument('--train', type=str, help='Train a specific model (name)')\n",
        "    parser.add_argument('--force', action='store_true', help='Force rebuild')\n",
        "    parser.add_argument('--serve', action='store_true', help='Launch Gradio UI')\n",
        "    args = parser.parse_known_args()[0]\n",
        "\n",
        "    if args.train:\n",
        "        print(\"Training:\", args.train)\n",
        "        train_model(args.train, csv_path=args.csv, force_retrain=args.force)\n",
        "    elif args.serve:\n",
        "        demo = build_gradio()\n",
        "        demo.launch()\n",
        "    else:\n",
        "        print(\"Script ready. Use --train <model_name> or --serve to launch the UI.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Script ready. Use --train <model_name> or --serve to launch the UI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlhTkazTP4T4",
        "outputId": "a4fddefa-9d32-4672-b051-72950800ff13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-6.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Collecting brotli>=1.1.0 (from gradio)\n",
            "  Downloading brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.124.2-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-1.0.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==2.0.1 (from gradio)\n",
            "  Downloading gradio_client-2.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<13.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (12.0.0)\n",
            "Collecting pydantic<=2.12.4,>=2.11.10 (from gradio)\n",
            "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Collecting safehttpx<0.2.0,>=0.1.7 (from gradio)\n",
            "  Downloading safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==2.0.1->gradio) (2025.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Collecting annotated-doc>=0.0.2 (from fastapi<1.0,>=0.115.2->gradio)\n",
            "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-6.1.0-py3-none-any.whl (23.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-2.0.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.124.2-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
            "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-1.0.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: pydub, brotli, uvicorn, tomlkit, shellingham, semantic-version, python-multipart, orjson, groovy, ffmpy, annotated-doc, aiofiles, starlette, pydantic, typer, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 25.1.0\n",
            "    Uninstalling aiofiles-25.1.0:\n",
            "      Successfully uninstalled aiofiles-25.1.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.12.5\n",
            "    Uninstalling pydantic-2.12.5:\n",
            "      Successfully uninstalled pydantic-2.12.5\n",
            "Successfully installed aiofiles-24.1.0 annotated-doc-0.0.4 brotli-1.2.0 fastapi-0.124.2 ffmpy-1.0.0 gradio-6.1.0 gradio-client-2.0.1 groovy-0.1.2 orjson-3.11.5 pydantic-2.12.4 pydub-0.25.1 python-multipart-0.0.20 safehttpx-0.1.7 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.50.0 tomlkit-0.13.3 typer-0.20.0 uvicorn-0.38.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY_1sCI5PV8T",
        "outputId": "464b7043-bf30-4df1-ff33-f2ad7bf16222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m826.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m149.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.4 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"cnn_fasttext_gensim\", force_retrain=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0GENNR0E0ss",
        "outputId": "229295fb-dc23-48ec-c70c-3f0bf853d78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for: cnn_fasttext_gensim\n",
            "Training FastText gensim model with vector_size=300...\n",
            "Saved FastText gensim model to /content/drive/MyDrive/10models_aio/fasttest_gensim_back/cnn_fasttext_gensim_fasttext.model\n",
            "Epoch 1/2\n",
            "   7836/Unknown \u001b[1m843s\u001b[0m 107ms/step - accuracy: 0.9669 - loss: 0.1159\n",
            "Epoch 1: saving model to /content/drive/MyDrive/10models_aio_back/cnn_fasttext_gensim/model_epoch-01-val_loss-0.0217.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.02169, saving model to /content/drive/MyDrive/10models_aio_back/cnn_fasttext_gensim/model_best.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7836/7836\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m885s\u001b[0m 113ms/step - accuracy: 0.9669 - loss: 0.1159 - val_accuracy: 0.9945 - val_loss: 0.0217\n",
            "Epoch 2/2\n",
            "\u001b[1m7835/7836\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9909 - loss: 0.0407\n",
            "Epoch 2: saving model to /content/drive/MyDrive/10models_aio_back/cnn_fasttext_gensim/model_epoch-02-val_loss-0.0194.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_loss improved from 0.02169 to 0.01943, saving model to /content/drive/MyDrive/10models_aio_back/cnn_fasttext_gensim/model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m7836/7836\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m876s\u001b[0m 112ms/step - accuracy: 0.9909 - loss: 0.0407 - val_accuracy: 0.9951 - val_loss: 0.0194\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Training finished for: cnn_fasttext_gensim\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"rnn_fasttext_gensim\", force_retrain=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kfwudqmCXrUL",
        "outputId": "39a29cea-59ce-4837-dae3-c7b2306d2689"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for: rnn_fasttext_gensim\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-3077149249.py\", line 1, in <cell line: 0>\n",
            "    train_model(\"rnn_fasttext_gensim\", force_retrain=True)\n",
            "  File \"/tmp/ipython-input-422517107.py\", line 446, in train_model\n",
            "    df = load_data(csv_path)\n",
            "         ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-422517107.py\", line 163, in load_data\n",
            "    df = pd.read_csv(CSV_FALLBACK)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 626, in _read\n",
            "    return parser.read(nrows)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1923, in read\n",
            "    ) = self._engine.read(  # type: ignore[attr-defined]\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\", line 234, in read\n",
            "    chunks = self._reader.read_low_memory(nrows)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"parsers.pyx\", line 838, in pandas._libs.parsers.TextReader.read_low_memory\n",
            "  File \"parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._read_rows\n",
            "  File \"parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._tokenize_rows\n",
            "  File \"parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status\n",
            "  File \"parsers.pyx\", line 2053, in pandas._libs.parsers.raise_parser_error\n",
            "  File \"<frozen codecs>\", line 319, in decode\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1714, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 970, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1016, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 427, in realpath\n",
            "  File \"<frozen posixpath>\", line 471, in _joinrealpath\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3077149249.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rnn_fasttext_gensim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_retrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-422517107.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, csv_path, force_retrain)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trade_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-422517107.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_FALLBACK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_FALLBACK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKPiImbs7HI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"cnn_fasttext_gensim\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "c56ef3a6-f62e-4931-dcc5-a419d6677e94",
        "id": "CvVfG_0n7Hpu"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for: cnn_fasttext_gensim\n",
            "Error training cnn_fasttext_gensim : operands could not be broadcast together with shapes (300,) (100,) (300,) \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (300,) (100,) (300,) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2312894507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_fasttext_gensim\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-422517107.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, csv_path, force_retrain)\u001b[0m\n\u001b[1;32m    591\u001b[0m           \u001b[0;31m# Load or train FastText gensim model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce_retrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m               \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fasttext_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fasttext_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fasttext_gensim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-422517107.py\u001b[0m in \u001b[0;36mload_fasttext_gensim\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_fasttext_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_to_word_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrethrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;34m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bucket'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;31m# should only exist in one place: the wv subcomponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1967\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;34m\"\"\"Handle special requirements of `.load()` protocol, usually up-converting older versions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m         \u001b[0;31m# for backward compatibility, add/rearrange properties from prior versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, fname, mmap, compress, subname)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s recursively from %s.* with mmap=%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mignore_deprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__numpys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m_load_specials\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecalc_char_ngram_buckets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vectors'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# recompose full-word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36madjust_vectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m             \u001b[0mngram_buckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuckets_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngram_buckets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_buckets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (300,) (100,) (300,) "
          ]
        }
      ]
    }
  ]
}