{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This file updated today to trace the gradio not visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470de33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified.py\n",
    "# Unified trainer & predictor: 10 models (CNN/RNN x word/char/combined/fasttext_keras/fasttext_gensim)\n",
    "# Save to /content/unified.py and run:\n",
    "#   !python /content/unified.py --train cnn_word --csv \"/content/drive/MyDrive/output_8_1_M_1_balanced_utf8.csv\"\n",
    "#   !python /content/unified.py --serve\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import datetime\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "                                     Dense, Dropout, concatenate, LSTM, Bidirectional)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# gensim FastText (real)\n",
    "from gensim.models import FastText\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Gradio\n",
    "import gradio as gr\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration (user-specified)\n",
    "# -----------------------------\n",
    "ROOT_MODEL_DIR = \"/10models\"\n",
    "CSV_FALLBACK = \"/output_8_1_M_1_balanced_utf8.csv\"\n",
    "\n",
    "# Paths for FastText storage (user-specified)\n",
    "FT_GENSIM_DIR = \"/10models/fasttest_gensim\"\n",
    "FT_KERAS_DIR = \"/10models/fasttest_keras\"\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"cnn_word\",\n",
    "    \"cnn_char\",\n",
    "    \"cnn_combined\",\n",
    "    \"cnn_fasttext_keras\",\n",
    "    \"cnn_fasttext_gensim\",\n",
    "    \"rnn_word\",\n",
    "    \"rnn_char\",\n",
    "    \"rnn_combined\",\n",
    "    \"rnn_fasttext_keras\",\n",
    "    \"rnn_fasttext_gensim\"\n",
    "]\n",
    "\n",
    "CFG = {\n",
    "    \"word\": {\"max_words\": 20000, \"max_len\": 25, \"embedding_dim\": 100},\n",
    "    \"char\": {\"max_chars\": 200, \"vocab_size\": 200, \"embedding_dim\": 64},\n",
    "    \"cnn\": {\"filters\": [2, 3, 4], \"num_filters\": 128, \"dropout\": 0.5},\n",
    "    \"rnn\": {\"rnn_units\": 128, \"dropout\": 0.5},\n",
    "    \"training\": {\"batch_size\": 64, \"epochs\": 15, \"validation_split\": 0.15},\n",
    "    # gensim FastText (real)\n",
    "    \"fasttext_gensim\": {\"vector_size\": 300, \"window\": 5, \"min_count\": 1, \"workers\": 4, \"epochs\": 10, \"sg\": 1},\n",
    "    # Keras-style fasttext embedding (trainable embedding layer, optional subword-like handling could be added)\n",
    "    \"fasttext_keras\": {\"embedding_dim\": 100}\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Ensure directories\n",
    "# -----------------------------\n",
    "def ensure_dirs():\n",
    "    os.makedirs(ROOT_MODEL_DIR, exist_ok=True)\n",
    "    os.makedirs(FT_GENSIM_DIR, exist_ok=True)\n",
    "    os.makedirs(FT_KERAS_DIR, exist_ok=True)\n",
    "    for mn in MODEL_NAMES:\n",
    "        os.makedirs(os.path.join(ROOT_MODEL_DIR, mn), exist_ok=True)\n",
    "\n",
    "ensure_dirs()\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & helpers\n",
    "# -----------------------------\n",
    "def model_folder(model_name):\n",
    "    p = os.path.join(ROOT_MODEL_DIR, model_name)\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def model_paths(model_name):\n",
    "    base = model_folder(model_name)\n",
    "    return {\n",
    "        \"base\": base,\n",
    "        \"model_best\": os.path.join(base, \"model_best.h5\"),\n",
    "        \"model_epoch_pattern\": os.path.join(base, \"model_epoch-{epoch:02d}-val_loss-{val_loss:.4f}.h5\"),\n",
    "        \"word_tokenizer\": os.path.join(base, \"word_tokenizer.json\"),\n",
    "        \"char_tokenizer\": os.path.join(base, \"char_tokenizer.json\"),\n",
    "        \"classes\": os.path.join(base, \"classes.npy\"),\n",
    "        \"fasttext_gensim\": os.path.join(FT_GENSIM_DIR, f\"{model_name}_fasttext.model\"),\n",
    "        \"fasttext_keras\": os.path.join(FT_KERAS_DIR, f\"{model_name}_ft_keras.json\"),\n",
    "        \"training_state\": os.path.join(base, \"training_state.json\"),\n",
    "        \"train_log\": os.path.join(base, \"train.log\")\n",
    "    }\n",
    "\n",
    "def save_json_atomic(obj, path):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def append_log(model_name, msg):\n",
    "    p = model_paths(model_name)[\"train_log\"]\n",
    "    ts = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
    "    with open(p, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{ts}] {msg}\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Training state (resume)\n",
    "# -----------------------------\n",
    "def load_training_state(model_name):\n",
    "    p = model_paths(model_name)[\"training_state\"]\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            return load_json(p)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_training_state(model_name, state):\n",
    "    p = model_paths(model_name)[\"training_state\"]\n",
    "    save_json_atomic(state, p)\n",
    "\n",
    "class EpochCheckpointCallback(Callback):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        state = load_training_state(self.model_name) or {}\n",
    "        if \"phases\" not in state:\n",
    "            state[\"phases\"] = {}\n",
    "        state[\"phases\"][self.model_name] = {\n",
    "            \"last_completed_epoch\": int(epoch) + 1,\n",
    "            \"updated_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"logs\": (logs or {})\n",
    "        }\n",
    "        save_training_state(self.model_name, state)\n",
    "\n",
    "# -----------------------------\n",
    "# Data loading & preprocessing\n",
    "# -----------------------------\n",
    "def simple_clean_text(text):\n",
    "    text = str(text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def load_data(csv_path=\"\"):\n",
    "    if csv_path and os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "    elif os.path.exists(CSV_FALLBACK):\n",
    "        df = pd.read_csv(CSV_FALLBACK)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"CSV not found at {csv_path} or fallback {CSV_FALLBACK}\")\n",
    "\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    if \"trade_name\" not in df.columns or \"reason\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'trade_name' and 'reason' columns\")\n",
    "    df = df[[\"trade_name\", \"reason\"]].dropna()\n",
    "    df[\"trade_name\"] = df[\"trade_name\"].astype(str).apply(simple_clean_text)\n",
    "    df[\"reason\"] = df[\"reason\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizers & char mapping\n",
    "# -----------------------------\n",
    "def build_word_tokenizer(texts, max_words):\n",
    "    tok = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tok.fit_on_texts(texts)\n",
    "    return tok\n",
    "\n",
    "def save_tokenizer_json(tokenizer, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tokenizer.to_json())\n",
    "\n",
    "def load_tokenizer_json(path):\n",
    "    from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return tokenizer_from_json(f.read())\n",
    "\n",
    "def build_char_tokenizer(texts, max_vocab=None):\n",
    "    chars = set()\n",
    "    for s in texts:\n",
    "        for ch in s:\n",
    "            chars.add(ch)\n",
    "    chars = sorted(chars)\n",
    "    if max_vocab is not None:\n",
    "        chars = chars[:max_vocab-2]\n",
    "    char_to_index = {ch: idx+2 for idx, ch in enumerate(chars)}\n",
    "    char_to_index[\"<PAD>\"] = 0\n",
    "    char_to_index[\"<OOV>\"] = 1\n",
    "    return char_to_index\n",
    "\n",
    "def save_char_tokenizer(char_map, vocab_size, path):\n",
    "    save_json_atomic({\"char_to_index\": char_map, \"vocab_size\": vocab_size}, path)\n",
    "\n",
    "def load_char_tokenizer(path):\n",
    "    data = load_json(path)\n",
    "    return data[\"char_to_index\"], data.get(\"vocab_size\", max(data[\"char_to_index\"].values()) + 1)\n",
    "\n",
    "def texts_to_char_sequences(texts, char_to_index, max_len):\n",
    "    seqs = []\n",
    "    pad = char_to_index.get(\"<PAD>\", 0)\n",
    "    oov = char_to_index.get(\"<OOV>\", 1)\n",
    "    for s in texts:\n",
    "        arr = [char_to_index.get(ch, oov) for ch in s]\n",
    "        if len(arr) < max_len:\n",
    "            arr = arr + [pad] * (max_len - len(arr))\n",
    "        else:\n",
    "            arr = arr[:max_len]\n",
    "        seqs.append(arr)\n",
    "    return np.array(seqs, dtype=np.int32)\n",
    "\n",
    "# -----------------------------\n",
    "# FastText (gensim) helpers\n",
    "# -----------------------------\n",
    "def tokenize_for_fasttext(text):\n",
    "    return simple_preprocess(text, deacc=False)\n",
    "\n",
    "def train_fasttext_gensim(sentences, path, ft_conf):\n",
    "    if not sentences or len(sentences) == 0:\n",
    "        raise ValueError(\"FastText requires non-empty sentences\")\n",
    "    model = FastText(vector_size=ft_conf[\"vector_size\"],\n",
    "                     window=ft_conf[\"window\"],\n",
    "                     min_count=ft_conf[\"min_count\"],\n",
    "                     workers=ft_conf[\"workers\"],\n",
    "                     sg=ft_conf.get(\"sg\", 1))\n",
    "    model.build_vocab(sentences=sentences)\n",
    "    model.train(sentences=sentences, total_examples=len(sentences), epochs=ft_conf[\"epochs\"])\n",
    "    model.save(path)\n",
    "    return model\n",
    "\n",
    "def load_fasttext_gensim(path):\n",
    "    return FastText.load(path)\n",
    "\n",
    "def text_to_word_vectors(ft_model, tokens, max_len):\n",
    "    vsz = ft_model.vector_size\n",
    "    vecs = []\n",
    "    for t in tokens[:max_len]:\n",
    "        try:\n",
    "            v = ft_model.wv.get_vector(t)\n",
    "        except Exception:\n",
    "            v = np.zeros(vsz, dtype=np.float32)\n",
    "        vecs.append(v)\n",
    "    if len(vecs) < max_len:\n",
    "        vecs.extend([np.zeros(vsz, dtype=np.float32)] * (max_len - len(vecs)))\n",
    "    return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Model builders\n",
    "# -----------------------------\n",
    "def build_word_cnn(max_words, max_len, embedding_dim, n_classes, filters, num_filters, dropout):\n",
    "    inp = Input(shape=(max_len,), name=\"word_input\")\n",
    "    emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len, name=\"word_emb\")(inp)\n",
    "    convs = []\n",
    "    for f in filters:\n",
    "        c = Conv1D(filters=num_filters, kernel_size=f, activation=\"relu\")(emb)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        convs.append(c)\n",
    "    x = concatenate(convs) if len(convs) > 1 else convs[0]\n",
    "    x = Dropout(dropout)(x)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_char_cnn(vocab_size, max_chars, embedding_dim, filters, num_filters, dropout, n_classes):\n",
    "    inp = Input(shape=(max_chars,), name=\"char_input\")\n",
    "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_chars, name=\"char_emb\")(inp)\n",
    "    convs = []\n",
    "    for f in filters:\n",
    "        c = Conv1D(filters=num_filters, kernel_size=f, activation=\"relu\")(emb)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        convs.append(c)\n",
    "    x = concatenate(convs) if len(convs) > 1 else convs[0]\n",
    "    x = Dropout(dropout)(x)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_combined_cnn(word_conf, char_conf, n_classes):\n",
    "    w_in = Input(shape=(word_conf[\"max_len\"],), name=\"word_input\")\n",
    "    w_emb = Embedding(input_dim=word_conf[\"max_words\"], output_dim=word_conf[\"embedding_dim\"], input_length=word_conf[\"max_len\"])(w_in)\n",
    "    w_convs = []\n",
    "    for f in CFG[\"cnn\"][\"filters\"]:\n",
    "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_emb)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        w_convs.append(c)\n",
    "    w_feat = concatenate(w_convs) if len(w_convs) > 1 else w_convs[0]\n",
    "    w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
    "\n",
    "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
    "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
    "    c_convs = []\n",
    "    for f in char_conf.get(\"filters\", [3,4,5]):\n",
    "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        c_convs.append(c)\n",
    "    c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
    "    c_feat = Dropout(char_conf.get(\"dropout\", CFG[\"char\"].get(\"dropout\", 0.5)))(c_feat)\n",
    "\n",
    "    merged = concatenate([w_feat, c_feat])\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
    "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_fasttext_combined_cnn(ft_embed_dim, word_max_len, char_conf, n_classes):\n",
    "    w_in = Input(shape=(word_max_len, ft_embed_dim), name=\"word_vec_input\")\n",
    "    convs = []\n",
    "    for f in CFG[\"cnn\"][\"filters\"]:\n",
    "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_in)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        convs.append(c)\n",
    "    w_feat = concatenate(convs) if len(convs) > 1 else convs[0]\n",
    "    w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
    "\n",
    "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
    "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
    "    c_convs = []\n",
    "    for f in char_conf.get(\"filters\", [3,4,5]):\n",
    "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        c_convs.append(c)\n",
    "    c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
    "    c_feat = Dropout(char_conf.get(\"dropout\", CFG[\"char\"].get(\"dropout\", 0.5)))(c_feat)\n",
    "\n",
    "    merged = concatenate([w_feat, c_feat])\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
    "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# RNN builders\n",
    "def build_word_rnn(max_words, max_len, embedding_dim, n_classes, rnn_units, dropout):\n",
    "    inp = Input(shape=(max_len,), name=\"word_input\")\n",
    "    emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len)(inp)\n",
    "    x = Bidirectional(LSTM(rnn_units))(emb)\n",
    "    x = Dropout(dropout)(x)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_char_rnn(vocab_size, max_chars, embedding_dim, rnn_units, dropout, n_classes):\n",
    "    inp = Input(shape=(max_chars,), name=\"char_input\")\n",
    "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_chars)(inp)\n",
    "    x = Bidirectional(LSTM(rnn_units))(emb)\n",
    "    x = Dropout(dropout)(x)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_combined_rnn(word_conf, char_conf, n_classes):\n",
    "    w_in = Input(shape=(word_conf[\"max_len\"],), name=\"word_input\")\n",
    "    w_emb = Embedding(input_dim=word_conf[\"max_words\"], output_dim=word_conf[\"embedding_dim\"], input_length=word_conf[\"max_len\"])(w_in)\n",
    "    w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_emb)\n",
    "    w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
    "\n",
    "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
    "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
    "    c_x = Bidirectional(LSTM(char_conf.get(\"rnn_units\", CFG[\"rnn\"][\"rnn_units\"])))(c_emb)\n",
    "    c_x = Dropout(char_conf.get(\"dropout\", CFG[\"rnn\"][\"dropout\"]))(c_x)\n",
    "\n",
    "    merged = concatenate([w_x, c_x])\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
    "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_fasttext_combined_rnn(ft_embed_dim, word_max_len, char_conf, n_classes):\n",
    "    w_in = Input(shape=(word_max_len, ft_embed_dim), name=\"word_vec_input\")\n",
    "    w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_in)\n",
    "    w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
    "\n",
    "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
    "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
    "    c_x = Bidirectional(LSTM(char_conf.get(\"rnn_units\", CFG[\"rnn\"][\"rnn_units\"])))(c_emb)\n",
    "    c_x = Dropout(char_conf.get(\"dropout\", CFG[\"rnn\"][\"dropout\"]))(c_x)\n",
    "\n",
    "    merged = concatenate([w_x, c_x])\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
    "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Central training function\n",
    "# -----------------------------\n",
    "def train_model(model_name, csv_path=\"\", force_retrain=False):\n",
    "    model_name = str(model_name).strip()\n",
    "    if model_name not in MODEL_NAMES:\n",
    "        raise ValueError(\"Unknown model: \" + model_name)\n",
    "    append_log(model_name, f\"=== START TRAIN [{model_name}] ===\")\n",
    "    print(f\"Starting training for: {model_name}\")\n",
    "\n",
    "    paths = model_paths(model_name)\n",
    "    df = load_data(csv_path)\n",
    "    texts = df[\"trade_name\"].tolist()\n",
    "\n",
    "    # labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[\"reason\"])\n",
    "    classes = le.classes_\n",
    "    n_classes = len(classes)\n",
    "    np.save(paths[\"classes\"], classes, allow_pickle=True)\n",
    "    y_cat = to_categorical(y, num_classes=n_classes)\n",
    "\n",
    "    state = load_training_state(model_name) or {}\n",
    "    phases = state.get(\"phases\", {})\n",
    "    last_completed = int(phases.get(model_name, {}).get(\"last_completed_epoch\", 0))\n",
    "    initial_epoch = last_completed\n",
    "    epochs = CFG[\"training\"][\"epochs\"]\n",
    "\n",
    "    cb_epoch = ModelCheckpoint(paths[\"model_epoch_pattern\"], save_best_only=False, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
    "    cb_best = ModelCheckpoint(paths[\"model_best\"], save_best_only=True, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
    "    cb_early = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1)\n",
    "    cb_state = EpochCheckpointCallback(model_name)\n",
    "\n",
    "    try:\n",
    "        # 1) cnn_word\n",
    "        if model_name == \"cnn_word\":\n",
    "            tok_path = paths[\"word_tokenizer\"]\n",
    "            if os.path.exists(tok_path) and not force_retrain:\n",
    "                word_tok = load_tokenizer_json(tok_path)\n",
    "            else:\n",
    "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
    "                save_tokenizer_json(word_tok, tok_path)\n",
    "            seqs = word_tok.texts_to_sequences(texts)\n",
    "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                model = build_word_cnn(CFG[\"word\"][\"max_words\"], CFG[\"word\"][\"max_len\"], CFG[\"word\"][\"embedding_dim\"], n_classes, CFG[\"cnn\"][\"filters\"], CFG[\"cnn\"][\"num_filters\"], CFG[\"cnn\"][\"dropout\"])\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit(X_word, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 2) cnn_char\n",
    "        elif model_name == \"cnn_char\":\n",
    "            char_path = paths[\"char_tokenizer\"]\n",
    "            if os.path.exists(char_path) and not force_retrain:\n",
    "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
    "            else:\n",
    "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
    "                vocab_size = max(char_map.values()) + 1\n",
    "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
    "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                filters = CFG[\"char\"].get(\"filters\", [3,4,5])\n",
    "                model = build_char_cnn(vocab_size, CFG[\"char\"][\"max_chars\"], CFG[\"char\"][\"embedding_dim\"], filters, CFG[\"cnn\"][\"num_filters\"], CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"]), n_classes)\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit(X_char, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 3) cnn_combined\n",
    "        elif model_name == \"cnn_combined\":\n",
    "            tok_path = paths[\"word_tokenizer\"]\n",
    "            if os.path.exists(tok_path) and not force_retrain:\n",
    "                word_tok = load_tokenizer_json(tok_path)\n",
    "            else:\n",
    "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
    "                save_tokenizer_json(word_tok, tok_path)\n",
    "            seqs = word_tok.texts_to_sequences(texts)\n",
    "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "            char_path = paths[\"char_tokenizer\"]\n",
    "            if os.path.exists(char_path) and not force_retrain:\n",
    "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
    "            else:\n",
    "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
    "                vocab_size = max(char_map.values()) + 1\n",
    "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
    "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                word_conf = {\"max_len\": CFG[\"word\"][\"max_len\"], \"max_words\": CFG[\"word\"][\"max_words\"], \"embedding_dim\": CFG[\"word\"][\"embedding_dim\"]}\n",
    "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"filters\": CFG[\"char\"].get(\"filters\",[3,4,5]), \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])}\n",
    "                model = build_combined_cnn(word_conf, char_conf, n_classes)\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 4) cnn_fasttext_keras\n",
    "        elif model_name == \"cnn_fasttext_keras\":\n",
    "            # Keras-style fasttext: we use a trainable Embedding on word indices (no gensim)\n",
    "            tok_path = paths[\"word_tokenizer\"]\n",
    "            if os.path.exists(tok_path) and not force_retrain:\n",
    "                word_tok = load_tokenizer_json(tok_path)\n",
    "            else:\n",
    "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
    "                save_tokenizer_json(word_tok, tok_path)\n",
    "            seqs = word_tok.texts_to_sequences(texts)\n",
    "            vocab_size = min(CFG[\"word\"][\"max_words\"], (len(word_tok.word_index) + 1))\n",
    "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "            # char tokenizer for combined branch\n",
    "            char_path = paths[\"char_tokenizer\"]\n",
    "            if os.path.exists(char_path) and not force_retrain:\n",
    "                char_map, _ = load_char_tokenizer(char_path)\n",
    "            else:\n",
    "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
    "                save_char_tokenizer(char_map, max(char_map.values())+1, char_path)\n",
    "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
    "            # Model: word branch uses embedding with embedding_dim = fasttext_keras embedding dim\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                # Build CNN that uses trainable embedding for words (fasttext-like)\n",
    "                w_in = Input(shape=(CFG[\"word\"][\"max_len\"],), name=\"word_input\")\n",
    "                w_emb = Embedding(input_dim=vocab_size, output_dim=CFG[\"fasttext_keras\"][\"embedding_dim\"], input_length=CFG[\"word\"][\"max_len\"])(w_in)\n",
    "                convs = []\n",
    "                for f in CFG[\"cnn\"][\"filters\"]:\n",
    "                    c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_emb)\n",
    "                    c = GlobalMaxPooling1D()(c)\n",
    "                    convs.append(c)\n",
    "                w_feat = concatenate(convs) if len(convs) > 1 else convs[0]\n",
    "                w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
    "\n",
    "                # char branch\n",
    "                c_in = Input(shape=(CFG[\"char\"][\"max_chars\"],), name=\"char_input\")\n",
    "                c_emb = Embedding(input_dim=max(char_map.values())+1, output_dim=CFG[\"char\"][\"embedding_dim\"], input_length=CFG[\"char\"][\"max_chars\"])(c_in)\n",
    "                c_convs = []\n",
    "                for f in CFG[\"char\"].get(\"filters\",[3,4,5]):\n",
    "                    c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
    "                    c = GlobalMaxPooling1D()(c)\n",
    "                    c_convs.append(c)\n",
    "                c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
    "                c_feat = Dropout(CFG[\"char\"].get(\"dropout\", 0.5))(c_feat)\n",
    "\n",
    "                merged = concatenate([w_feat, c_feat])\n",
    "                merged = Dropout(0.5)(merged)\n",
    "                out = Dense(n_classes, activation=\"softmax\")(merged)\n",
    "                model = Model(inputs=[w_in, c_in], outputs=out)\n",
    "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 5) cnn_fasttext_gensim\n",
    "        elif model_name == \"cnn_fasttext_gensim\":\n",
    "            ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
    "            sentences = [tokenize_for_fasttext(t) for t in texts]\n",
    "            if os.path.exists(ft_path) and not force_retrain:\n",
    "                ft = load_fasttext_gensim(ft_path)\n",
    "            else:\n",
    "                ft = train_fasttext_gensim(sentences, ft_path, CFG[\"fasttext_gensim\"])\n",
    "            embed_dim = ft.vector_size\n",
    "            max_len = CFG[\"word\"][\"max_len\"]\n",
    "            X_word = np.stack([text_to_word_vectors(ft, tokenize_for_fasttext(t), max_len) for t in texts], axis=0)\n",
    "            char_path = paths[\"char_tokenizer\"]\n",
    "            if os.path.exists(char_path) and not force_retrain:\n",
    "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
    "            else:\n",
    "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
    "                vocab_size = max(char_map.values()) + 1\n",
    "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
    "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"filters\": CFG[\"char\"].get(\"filters\",[3,4,5]), \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])}\n",
    "                model = build_fasttext_combined_cnn(embed_dim, CFG[\"word\"][\"max_len\"], char_conf, n_classes)\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 6) rnn_word\n",
    "        elif model_name == \"rnn_word\":\n",
    "            tok_path = paths[\"word_tokenizer\"]\n",
    "            if os.path.exists(tok_path) and not force_retrain:\n",
    "                word_tok = load_tokenizer_json(tok_path)\n",
    "            else:\n",
    "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
    "                save_tokenizer_json(word_tok, tok_path)\n",
    "            seqs = word_tok.texts_to_sequences(texts)\n",
    "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                model = build_word_rnn(CFG[\"word\"][\"max_words\"], CFG[\"word\"][\"max_len\"], CFG[\"word\"][\"embedding_dim\"], n_classes, CFG[\"rnn\"][\"rnn_units\"], CFG[\"rnn\"][\"dropout\"])\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit(X_word, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 7) rnn_char\n",
    "        elif model_name == \"rnn_char\":\n",
    "            char_path = paths[\"char_tokenizer\"]\n",
    "            if os.path.exists(char_path) and not force_retrain:\n",
    "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
    "            else:\n",
    "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
    "                vocab_size = max(char_map.values()) + 1\n",
    "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
    "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                model = build_char_rnn(vocab_size, CFG[\"char\"][\"max_chars\"], CFG[\"char\"][\"embedding_dim\"], CFG[\"rnn\"][\"rnn_units\"], CFG[\"rnn\"][\"dropout\"], n_classes)\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit(X_char, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 8) rnn_combined\n",
    "        elif model_name == \"rnn_combined\":\n",
    "            tok_path = paths[\"word_tokenizer\"]\n",
    "            if os.path.exists(tok_path) and not force_retrain:\n",
    "                word_tok = load_tokenizer_json(tok_path)\n",
    "            else:\n",
    "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
    "                save_tokenizer_json(word_tok, tok_path)\n",
    "            seqs = word_tok.texts_to_sequences(texts)\n",
    "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "            char_path = paths[\"char_tokenizer\"]\n",
    "            if os.path.exists(char_path) and not force_retrain:\n",
    "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
    "            else:\n",
    "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
    "                vocab_size = max(char_map.values()) + 1\n",
    "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
    "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                word_conf = {\"max_len\": CFG[\"word\"][\"max_len\"], \"max_words\": CFG[\"word\"][\"max_words\"], \"embedding_dim\": CFG[\"word\"][\"embedding_dim\"]}\n",
    "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"rnn_units\": CFG[\"rnn\"][\"rnn_units\"], \"dropout\": CFG[\"rnn\"][\"dropout\"]}\n",
    "                model = build_combined_rnn(word_conf, char_conf, n_classes)\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 9) rnn_fasttext_keras\n",
    "        elif model_name == \"rnn_fasttext_keras\":\n",
    "            tok_path = paths[\"word_tokenizer\"]\n",
    "            if os.path.exists(tok_path) and not force_retrain:\n",
    "                word_tok = load_tokenizer_json(tok_path)\n",
    "            else:\n",
    "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
    "                save_tokenizer_json(word_tok, tok_path)\n",
    "            seqs = word_tok.texts_to_sequences(texts)\n",
    "            vocab_size = min(CFG[\"word\"][\"max_words\"], (len(word_tok.word_index) + 1))\n",
    "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "            char_path = paths[\"char_tokenizer\"]\n",
    "            if os.path.exists(char_path) and not force_retrain:\n",
    "                char_map, _ = load_char_tokenizer(char_path)\n",
    "            else:\n",
    "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
    "                save_char_tokenizer(char_map, max(char_map.values())+1, char_path)\n",
    "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                # RNN combined with trainable embedding for words\n",
    "                w_in = Input(shape=(CFG[\"word\"][\"max_len\"],), name=\"word_input\")\n",
    "                w_emb = Embedding(input_dim=vocab_size, output_dim=CFG[\"fasttext_keras\"][\"embedding_dim\"], input_length=CFG[\"word\"][\"max_len\"])(w_in)\n",
    "                w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_emb)\n",
    "                w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
    "\n",
    "                c_in = Input(shape=(CFG[\"char\"][\"max_chars\"],), name=\"char_input\")\n",
    "                c_emb = Embedding(input_dim=max(char_map.values())+1, output_dim=CFG[\"char\"][\"embedding_dim\"], input_length=CFG[\"char\"][\"max_chars\"])(c_in)\n",
    "                c_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(c_emb)\n",
    "                c_x = Dropout(CFG[\"rnn\"][\"dropout\"])(c_x)\n",
    "\n",
    "                merged = concatenate([w_x, c_x])\n",
    "                merged = Dropout(0.5)(merged)\n",
    "                out = Dense(n_classes, activation=\"softmax\")(merged)\n",
    "                model = Model(inputs=[w_in, c_in], outputs=out)\n",
    "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        # 10) rnn_fasttext_gensim\n",
    "        elif model_name == \"rnn_fasttext_gensim\":\n",
    "            ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
    "            sentences = [tokenize_for_fasttext(t) for t in texts]\n",
    "            if os.path.exists(ft_path) and not force_retrain:\n",
    "                ft = load_fasttext_gensim(ft_path)\n",
    "            else:\n",
    "                ft = train_fasttext_gensim(sentences, ft_path, CFG[\"fasttext_gensim\"])\n",
    "            embed_dim = ft.vector_size\n",
    "            max_len = CFG[\"word\"][\"max_len\"]\n",
    "            X_word = np.stack([text_to_word_vectors(ft, tokenize_for_fasttext(t), max_len) for t in texts], axis=0)\n",
    "            char_path = paths[\"char_tokenizer\"]\n",
    "            if os.path.exists(char_path) and not force_retrain:\n",
    "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
    "            else:\n",
    "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
    "                vocab_size = max(char_map.values()) + 1\n",
    "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
    "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
    "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
    "                model = load_model(paths[\"model_best\"])\n",
    "            else:\n",
    "                model = build_fasttext_combined_rnn(embed_dim, CFG[\"word\"][\"max_len\"], {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"rnn_units\": CFG[\"rnn\"][\"rnn_units\"], \"dropout\": CFG[\"rnn\"][\"dropout\"]}, n_classes)\n",
    "            if initial_epoch < epochs:\n",
    "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=[cb_epoch, cb_best, cb_early, cb_state], verbose=2)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unhandled model: \" + model_name)\n",
    "\n",
    "    except Exception as exc:\n",
    "        tb = traceback.format_exc()\n",
    "        append_log(model_name, f\"TRAIN ERROR: {str(exc)}\\n{tb}\")\n",
    "        print(\"Error training\", model_name, \":\", str(exc))\n",
    "        raise\n",
    "\n",
    "    append_log(model_name, f\"=== FINISHED TRAIN [{model_name}] ===\")\n",
    "    print(f\"Training finished for: {model_name}\")\n",
    "    return True\n",
    "\n",
    "# -----------------------------\n",
    "# Resources & prediction\n",
    "# -----------------------------\n",
    "def load_resources_for_model(model_name):\n",
    "    if model_name not in MODEL_NAMES:\n",
    "        raise ValueError(\"Unknown model: \" + model_name)\n",
    "    paths = model_paths(model_name)\n",
    "    if not os.path.exists(paths[\"classes\"]):\n",
    "        raise FileNotFoundError(\"Classes file missing. Train first.\")\n",
    "    classes = np.load(paths[\"classes\"], allow_pickle=True)\n",
    "    if not os.path.exists(paths[\"model_best\"]):\n",
    "        raise FileNotFoundError(\"Best model missing. Train first.\")\n",
    "    model = load_model(paths[\"model_best\"])\n",
    "    res = {\"classes\": classes, \"model\": model}\n",
    "\n",
    "    if model_name in [\"cnn_word\", \"rnn_word\", \"cnn_combined\", \"rnn_combined\", \"cnn_fasttext_keras\", \"rnn_fasttext_keras\"]:\n",
    "        if not os.path.exists(paths[\"word_tokenizer\"]):\n",
    "            raise FileNotFoundError(\"Word tokenizer missing.\")\n",
    "        res[\"word_tokenizer\"] = load_tokenizer_json(paths[\"word_tokenizer\"])\n",
    "\n",
    "    if model_name in [\"cnn_char\", \"rnn_char\", \"cnn_combined\", \"rnn_combined\", \"cnn_fasttext_keras\", \"cnn_fasttext_gensim\", \"rnn_fasttext_keras\", \"rnn_fasttext_gensim\"]:\n",
    "        if not os.path.exists(paths[\"char_tokenizer\"]):\n",
    "            raise FileNotFoundError(\"Char tokenizer missing.\")\n",
    "        char_map, _ = load_char_tokenizer(paths[\"char_tokenizer\"])\n",
    "        res[\"char_map\"] = char_map\n",
    "\n",
    "    if model_name in [\"cnn_fasttext_gensim\", \"rnn_fasttext_gensim\", \"cnn_fasttext_gensim\"]:\n",
    "        ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
    "        if not os.path.exists(ft_path):\n",
    "            raise FileNotFoundError(\"FastText gensim model missing.\")\n",
    "        res[\"fasttext\"] = load_fasttext_gensim(ft_path)\n",
    "\n",
    "    return res\n",
    "\n",
    "def predict_for_model(model_name, text, resources):\n",
    "    t = simple_clean_text(text)\n",
    "    model = resources[\"model\"]\n",
    "    classes = resources[\"classes\"]\n",
    "    if model_name in [\"cnn_word\", \"rnn_word\"]:\n",
    "        tok = resources[\"word_tokenizer\"]\n",
    "        seq = tok.texts_to_sequences([t])\n",
    "        x = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "        preds = model.predict(x, verbose=0)\n",
    "    elif model_name in [\"cnn_char\", \"rnn_char\"]:\n",
    "        cm = resources[\"char_map\"]\n",
    "        x = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
    "        preds = model.predict(x, verbose=0)\n",
    "    elif model_name in [\"cnn_combined\", \"rnn_combined\"]:\n",
    "        tok = resources[\"word_tokenizer\"]\n",
    "        seq = tok.texts_to_sequences([t])\n",
    "        xw = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "        cm = resources[\"char_map\"]\n",
    "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
    "        preds = model.predict([xw, xc], verbose=0)\n",
    "    elif model_name in [\"cnn_fasttext_keras\", \"rnn_fasttext_keras\"]:\n",
    "        tok = resources[\"word_tokenizer\"]\n",
    "        seq = tok.texts_to_sequences([t])\n",
    "        xw = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
    "        cm = resources[\"char_map\"]\n",
    "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
    "        preds = model.predict([xw, xc], verbose=0)\n",
    "    elif model_name in [\"cnn_fasttext_gensim\", \"rnn_fasttext_gensim\"]:\n",
    "        ft = resources[\"fasttext\"]\n",
    "        tokens = tokenize_for_fasttext(t)\n",
    "        xw = np.expand_dims(text_to_word_vectors(ft, tokens, CFG[\"word\"][\"max_len\"]), axis=0)\n",
    "        cm = resources[\"char_map\"]\n",
    "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
    "        preds = model.predict([xw, xc], verbose=0)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model: \" + model_name)\n",
    "\n",
    "    idx = int(np.argmax(preds, axis=1)[0])\n",
    "    prob = float(np.max(preds))\n",
    "    label = str(classes[idx])\n",
    "    accepted = (label == \"ኖርማል\")\n",
    "    return {\"label\": label, \"probability\": prob, \"accepted\": accepted}\n",
    "\n",
    "# -----------------------------\n",
    "# Gradio UI\n",
    "# -----------------------------\n",
    "def build_gradio():\n",
    "    def on_train(model_name, csv_path, force):\n",
    "        try:\n",
    "            train_model(model_name, csv_path=csv_path.strip() if csv_path else \"\", force_retrain=force)\n",
    "            return f\"Training finished for: {model_name}\"\n",
    "        except Exception as e:\n",
    "            return \"ERROR: \" + str(e)\n",
    "\n",
    "    def on_predict(model_name, name):\n",
    "        try:\n",
    "            res = load_resources_for_model(model_name)\n",
    "            r = predict_for_model(model_name, name, res)\n",
    "            label = r[\"label\"]\n",
    "            conf = f\"{r['probability']*100:.2f}%\"\n",
    "            st = \"ACCEPTED ✅\" if r[\"accepted\"] else \"REJECTED ❌\"\n",
    "            return label, conf, st\n",
    "        except Exception as e:\n",
    "            return \"ERROR: \" + str(e), \"\", \"\"\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## Unified Models — select a model, train/resume, or predict\")\n",
    "        with gr.Row():\n",
    "            model_select = gr.Dropdown(MODEL_NAMES, value=MODEL_NAMES[0], label=\"Model\")\n",
    "            csv_input = gr.Textbox(label=\"CSV path (leave empty to use fallback)\", value=\"\")\n",
    "        with gr.Row():\n",
    "            train_btn = gr.Button(\"Train / Resume Selected Model\")\n",
    "            force_chk = gr.Checkbox(label=\"Force rebuild (delete/load fresh)\", value=False)\n",
    "            status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "        with gr.Row():\n",
    "            name_input = gr.Textbox(label=\"Proposed Trade Name\")\n",
    "            predict_btn = gr.Button(\"Predict\")\n",
    "        with gr.Row():\n",
    "            out_label = gr.Textbox(label=\"Predicted Reason\")\n",
    "            out_conf = gr.Textbox(label=\"Confidence\")\n",
    "            out_status = gr.Textbox(label=\"Decision\")\n",
    "\n",
    "        train_btn.click(on_train, inputs=[model_select, csv_input, force_chk], outputs=[status])\n",
    "        predict_btn.click(on_predict, inputs=[model_select, name_input], outputs=[out_label, out_conf, out_status])\n",
    "\n",
    "    return demo\n",
    "\n",
    "# -----------------------------\n",
    "# CLI\n",
    "# -----------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--csv', default='', help='Path to CSV dataset (optional)')\n",
    "    parser.add_argument('--train', type=str, help='Train a specific model (name)')\n",
    "    parser.add_argument('--force', action='store_true', help='Force rebuild')\n",
    "    parser.add_argument('--serve', action='store_true', help='Launch Gradio UI')\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    if args.train:\n",
    "        print(\"Training:\", args.train)\n",
    "        train_model(args.train, csv_path=args.csv, force_retrain=args.force)\n",
    "    elif args.serve:\n",
    "        demo = build_gradio()\n",
    "        demo.launch()\n",
    "    else:\n",
    "        print(\"Script ready. Use --train <model_name> or --serve to launch the UI.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f4c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
