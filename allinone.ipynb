{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyasu-taye/tdn_complaince_detection_models/blob/main/allinone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b4894f",
      "metadata": {
        "id": "84b4894f"
      },
      "outputs": [],
      "source": [
        "### This file updated today to trace the gradio not visible"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-gEjy_AMW2r",
        "outputId": "243db617-abf1-483e-c74e-43ed1107b244"
      },
      "id": "j-gEjy_AMW2r",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bf570e9"
      },
      "source": [
        "# Task\n",
        "The plan is to enhance the model training process by adding advanced metrics and improving logging. First, I will define a custom F1-score metric and a `BatchMetricsCallback` class to display batch-level training metrics. Then, I will update all `model.compile` calls in the `build_*_model` functions to include precision, recall, and the custom F1-score metric. Next, I will integrate the `BatchMetricsCallback` by instantiating it and adding it to the list of callbacks used in all `model.fit` calls, ensuring `verbose=0` is set in `model.fit` to let the custom callback manage output. Finally, I will verify and correct any minor errors in saving the word tokenizer for the `rnn_fasttext_keras` model, though a preliminary check suggests it might already be correct."
      ],
      "id": "4bf570e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e2f51f"
      },
      "source": [
        "## Add_F1_Metric_and_BatchMetricsCallback\n",
        "\n",
        "### Subtask:\n",
        "Define a custom F1-score metric function and a `BatchMetricsCallback` class to display batch-level training metrics.\n"
      ],
      "id": "15e2f51f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7492c463"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the custom F1-score metric functions (precision, recall, f1_score) and the `BatchMetricsCallback` class as instructed by the subtask. This involves importing the Keras backend, implementing the metric calculations with division-by-zero handling, and overriding the batch-end methods in the callback to display metrics.\n",
        "\n"
      ],
      "id": "7492c463"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ddfea54"
      },
      "source": [
        "### This file updated today to trace the gradio not visible\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision_val = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision_val\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall_val = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall_val\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    f1_val = 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "    return f1_val\n",
        "\n",
        "class BatchMetricsCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if logs is None: logs = {}\n",
        "        epoch = self.model.history.epoch[-1] if self.model.history.epoch else 0\n",
        "        metrics_str = \" \".join([f\"{k}: {v:.4f}\" for k, v in logs.items()])\n",
        "        print(f\"\\rEpoch {epoch+1} - Batch {batch+1} (train): {metrics_str}\", end=\"\")\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        if logs is None: logs = {}\n",
        "        epoch = self.model.history.epoch[-1] if self.model.history.epoch else 0\n",
        "        metrics_str = \" \".join([f\"{k}: {v:.4f}\" for k, v in logs.items()])\n",
        "        print(f\"\\rEpoch {epoch+1} - Batch {batch+1} (val): {metrics_str}\", end=\"\")"
      ],
      "id": "4ddfea54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59d1ebc2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update all `model.compile` calls in the `build_*_model` functions to include precision, recall, and f1_score metrics, and then integrate the `BatchMetricsCallback` by adding it to the list of callbacks used in all `model.fit` calls, ensuring `verbose=0` is set in `model.fit`. This involves rewriting the `train_model` function and all model building functions to incorporate these changes.\n",
        "\n"
      ],
      "id": "59d1ebc2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e9b928"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block contained a `SyntaxError` due to unmatched parentheses in the `build_combined_rnn` and `build_fasttext_combined_rnn` functions. I will correct these extra closing parentheses to resolve the error.\n",
        "\n"
      ],
      "id": "d3e9b928"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1876375"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block contained multiple `SyntaxError`s due to unmatched parentheses in the `build_combined_rnn`, `build_fasttext_combined_rnn`, and the inline model definition for `rnn_fasttext_keras` within `train_model` functions. I will correct these extra closing parentheses to resolve the errors.\n",
        "\n"
      ],
      "id": "c1876375"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f758e7e",
        "outputId": "32087400-fccf-4faf-eb19-731510d58041"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "import datetime\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Install gensim if not already present\n",
        "!pip install gensim # Uncommented to ensure gensim is installed\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
        "                                     Dense, Dropout, concatenate, LSTM, Bidirectional)\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K # Import Keras backend for custom metrics\n",
        "\n",
        "# gensim FastText (real)\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Gradio\n",
        "import gradio as gr\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration (user-specified)\n",
        "# -----------------------------\n",
        "ROOT_MODEL_DIR = \"/content/drive/MyDrive/10models_aio\"\n",
        "CSV_FALLBACK = \"/content/drive/MyDrive/output_8_1_M_1_balanced_utf8.csv\"\n",
        "\n",
        "# Paths for FastText storage (user-specified)\n",
        "FT_GENSIM_DIR = \"/content/drive/MyDrive/10models_aio/fasttest_gensim\"\n",
        "FT_KERAS_DIR = \"/content/drive/MyDrive/10models_aio/fasttest_keras\"\n",
        "\n",
        "MODEL_NAMES = [\n",
        "    \"cnn_word\",\n",
        "    \"cnn_char\",\n",
        "    \"cnn_combined\",\n",
        "    \"cnn_fasttext_keras\",\n",
        "    \"cnn_fasttext_gensim\",\n",
        "    \"rnn_word\",\n",
        "    \"rnn_char\",\n",
        "    \"rnn_combined\",\n",
        "    \"rnn_fasttext_keras\",\n",
        "    \"rnn_fasttext_gensim\"\n",
        "]\n",
        "\n",
        "CFG = {\n",
        "    \"word\": {\"max_words\": 20000, \"max_len\": 25, \"embedding_dim\": 100},\n",
        "    \"char\": {\"max_chars\": 200, \"vocab_size\": 200, \"embedding_dim\": 64},\n",
        "    \"cnn\": {\"filters\": [2, 3, 4], \"num_filters\": 128, \"dropout\": 0.5},\n",
        "    \"rnn\": {\"rnn_units\": 128, \"dropout\": 0.5},\n",
        "    \"training\": {\"batch_size\": 64, \"epochs\": 2, \"validation_split\": 0.15},\n",
        "    # gensim FastText (real)\n",
        "    \"fasttext_gensim\": {\"vector_size\": 300, \"window\": 5, \"min_count\": 1, \"workers\": 4, \"epochs\": 2, \"sg\": 1},\n",
        "    # Keras-style fasttext embedding (trainable embedding layer, optional subword-like handling could be added)\n",
        "    \"fasttext_keras\": {\"embedding_dim\": 100}\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Ensure directories\n",
        "# -----------------------------\n",
        "def ensure_dirs():\n",
        "    os.makedirs(ROOT_MODEL_DIR, exist_ok=True)\n",
        "    os.makedirs(FT_GENSIM_DIR, exist_ok=True)\n",
        "    os.makedirs(FT_KERAS_DIR, exist_ok=True)\n",
        "    for mn in MODEL_NAMES:\n",
        "        os.makedirs(os.path.join(ROOT_MODEL_DIR, mn), exist_ok=True)\n",
        "\n",
        "ensure_dirs()\n",
        "\n",
        "# -----------------------------\n",
        "# Paths & helpers\n",
        "# -----------------------------\n",
        "def model_folder(model_name):\n",
        "    p = os.path.join(ROOT_MODEL_DIR, model_name)\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def model_paths(model_name):\n",
        "    base = model_folder(model_name)\n",
        "    return {\n",
        "        \"base\": base,\n",
        "        \"model_best\": os.path.join(base, \"model_best.h5\"),\n",
        "        \"model_epoch_pattern\": os.path.join(base, \"model_epoch-{epoch:02d}-val_loss-{val_loss:.4f}.h5\"),\n",
        "        \"word_tokenizer\": os.path.join(base, \"word_tokenizer.json\"),\n",
        "        \"char_tokenizer\": os.path.join(base, \"char_tokenizer.json\"),\n",
        "        \"classes\": os.path.join(base, \"classes.npy\"),\n",
        "        \"fasttext_gensim\": os.path.join(FT_GENSIM_DIR, f\"{model_name}_fasttext.model\"),\n",
        "        \"fasttext_keras\": os.path.join(FT_KERAS_DIR, f\"{model_name}_ft_keras.json\"),\n",
        "        \"training_state\": os.path.join(base, \"training_state.json\"),\n",
        "        \"train_log\": os.path.join(base, \"train.log\")\n",
        "    }\n",
        "\n",
        "def save_json_atomic(obj, path):\n",
        "    tmp = path + \".tmp\"\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def load_json(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def append_log(model_name, msg):\n",
        "    p = model_paths(model_name)[\"train_log\"]\n",
        "    ts = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
        "    with open(p, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"[{ts}] {msg}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Training state (resume)\n",
        "# -----------------------------\n",
        "def load_training_state(model_name):\n",
        "    p = model_paths(model_name)[\"training_state\"]\n",
        "    if os.path.exists(p):\n",
        "        try:\n",
        "            return load_json(p)\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def save_training_state(model_name, state):\n",
        "    p = model_paths(model_name)[\"training_state\"]\n",
        "    save_json_atomic(state, p)\n",
        "\n",
        "class EpochCheckpointCallback(Callback):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        state = load_training_state(self.model_name) or {}\n",
        "        if \"phases\" not in state:\n",
        "            state[\"phases\"] = {}\n",
        "        state[\"phases\"][self.model_name] = {\n",
        "            \"last_completed_epoch\": int(epoch) + 1,\n",
        "            \"updated_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"logs\": (logs or {})\n",
        "        }\n",
        "        save_training_state(self.model_name, state)\n",
        "\n",
        "# -----------------------------\n",
        "# Data loading & preprocessing\n",
        "# -----------------------------\n",
        "def simple_clean_text(text):\n",
        "    text = str(text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def load_data(csv_path=\"\"):\n",
        "    if csv_path and os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "    elif os.path.exists(CSV_FALLBACK):\n",
        "        df = pd.read_csv(CSV_FALLBACK)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"CSV not found at {csv_path} or fallback {CSV_FALLBACK}\")\n",
        "\n",
        "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
        "    if \"trade_name\" not in df.columns or \"reason\" not in df.columns:\n",
        "        raise ValueError(\"CSV must contain 'trade_name' and 'reason' columns\")\n",
        "    df = df[[\"trade_name\", \"reason\"]].dropna()\n",
        "    df[\"trade_name\"] = df[\"trade_name\"].astype(str).apply(simple_clean_text)\n",
        "    df[\"reason\"] = df[\"reason\"].astype(str).str.strip()\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenizers & char mapping\n",
        "# -----------------------------\n",
        "def build_word_tokenizer(texts, max_words):\n",
        "    tok = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "    tok.fit_on_texts(texts)\n",
        "    return tok\n",
        "\n",
        "def save_tokenizer_json(tokenizer, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(tokenizer.to_json())\n",
        "\n",
        "def load_tokenizer_json(path):\n",
        "    from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return tokenizer_from_json(f.read())\n",
        "\n",
        "def build_char_tokenizer(texts, max_vocab=None):\n",
        "    chars = set()\n",
        "    for s in texts:\n",
        "        for ch in s:\n",
        "            chars.add(ch)\n",
        "    chars = sorted(chars)\n",
        "    if max_vocab is not None:\n",
        "        chars = chars[:max_vocab-2]\n",
        "    char_to_index = {ch: idx+2 for idx, ch in enumerate(chars)}\n",
        "    char_to_index[\"<PAD>\"] = 0\n",
        "    char_to_index[\"<OOV>\"] = 1\n",
        "    return char_to_index\n",
        "\n",
        "def save_char_tokenizer(char_map, vocab_size, path):\n",
        "    save_json_atomic({\"char_to_index\": char_map, \"vocab_size\": vocab_size}, path)\n",
        "\n",
        "def load_char_tokenizer(path):\n",
        "    data = load_json(path)\n",
        "    return data[\"char_to_index\"], data.get(\"vocab_size\", max(data[\"char_to_index\"].values()) + 1)\n",
        "\n",
        "def texts_to_char_sequences(texts, char_to_index, max_len):\n",
        "    seqs = []\n",
        "    pad = char_to_index.get(\"<PAD>\", 0)\n",
        "    oov = char_to_index.get(\"<OOV>\", 1)\n",
        "    for s in texts:\n",
        "        arr = [char_to_index.get(ch, oov) for ch in s]\n",
        "        if len(arr) < max_len:\n",
        "            arr = arr + [pad] * (max_len - len(arr))\n",
        "        else:\n",
        "            arr = arr[:max_len]\n",
        "        seqs.append(arr)\n",
        "    return np.array(seqs, dtype=np.int32)\n",
        "\n",
        "# -----------------------------\n",
        "# FastText (gensim) helpers\n",
        "# -----------------------------\n",
        "def tokenize_for_fasttext(text):\n",
        "    return simple_preprocess(text, deacc=False)\n",
        "\n",
        "def train_fasttext_gensim(sentences, path, ft_conf):\n",
        "    if not sentences or len(sentences) == 0:\n",
        "        raise ValueError(\"FastText requires non-empty sentences\")\n",
        "    model = FastText(vector_size=ft_conf[\"vector_size\"],\n",
        "                     window=ft_conf[\"window\"],\n",
        "                     min_count=ft_conf[\"min_count\"],\n",
        "                     workers=ft_conf[\"workers\"],\n",
        "                     sg=ft_conf.get(\"sg\", 1))\n",
        "    model.build_vocab(sentences=sentences)\n",
        "    model.train(sentences=sentences, total_examples=len(sentences), epochs=ft_conf[\"epochs\"])\n",
        "    model.save(path)\n",
        "    return model\n",
        "\n",
        "def load_fasttext_gensim(path):\n",
        "    return FastText.load(path)\n",
        "\n",
        "def text_to_word_vectors(ft_model, tokens, max_len):\n",
        "    vsz = ft_model.vector_size\n",
        "    vecs = []\n",
        "    for t in tokens[:max_len]:\n",
        "        try:\n",
        "            v = ft_model.wv.get_vector(t)\n",
        "        except Exception:\n",
        "            v = np.zeros(vsz, dtype=np.float32)\n",
        "        vecs.append(v)\n",
        "    if len(vecs) < max_len:\n",
        "        vecs.extend([np.zeros(vsz, dtype=np.float32)] * (max_len - len(vecs)))\n",
        "    return np.array(vecs, dtype=np.float32)\n",
        "\n",
        "# Custom F1-score metrics\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision_val = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision_val\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall_val = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall_val\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    f1_val = 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "    return f1_val\n",
        "\n",
        "# Custom Callback for batch metrics\n",
        "class BatchMetricsCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if logs is None: logs = {}\n",
        "        epoch = self.model.history.epoch[-1] if self.model.history.epoch else 0\n",
        "        metrics_str = \" \".join([f\"{k}: {v:.4f}\" for k, v in logs.items()])\n",
        "        print(f\"\\rEpoch {epoch+1} - Batch {batch+1} (train): {metrics_str}\", end=\"\")\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        if logs is None: logs = {}\n",
        "        epoch = self.model.history.epoch[-1] if self.model.history.epoch else 0\n",
        "        metrics_str = \" \".join([f\"{k}: {v:.4f}\" for k, v in logs.items()])\n",
        "        print(f\"\\rEpoch {epoch+1} - Batch {batch+1} (val): {metrics_str}\", end=\"\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model builders\n",
        "# -----------------------------\n",
        "def build_word_cnn(max_words, max_len, embedding_dim, n_classes, filters, num_filters, dropout):\n",
        "    inp = Input(shape=(max_len,), name=\"word_input\")\n",
        "    emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len, name=\"word_emb\")(inp)\n",
        "    convs = []\n",
        "    for f in filters:\n",
        "        c = Conv1D(filters=num_filters, kernel_size=f, activation=\"relu\")(emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    x = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "    return model\n",
        "\n",
        "def build_char_cnn(vocab_size, max_chars, embedding_dim, filters, num_filters, dropout, n_classes):\n",
        "    inp = Input(shape=(max_chars,), name=\"char_input\")\n",
        "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_chars, name=\"char_emb\")(inp)\n",
        "    convs = []\n",
        "    for f in filters:\n",
        "        c = Conv1D(filters=num_filters, kernel_size=f, activation=\"relu\")(emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    x = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "    return model\n",
        "\n",
        "def build_combined_cnn(word_conf, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_conf[\"max_len\"],), name=\"word_input\")\n",
        "    w_emb = Embedding(input_dim=word_conf[\"max_words\"], output_dim=word_conf[\"embedding_dim\"], input_length=word_conf[\"max_len\"])(w_in)\n",
        "    w_convs = []\n",
        "    for f in CFG[\"cnn\"][\"filters\"]:\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        w_convs.append(c)\n",
        "    w_feat = concatenate(w_convs) if len(w_convs) > 1 else w_convs[0]\n",
        "    w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_convs = []\n",
        "    for f in char_conf.get(\"filters\", [3,4,5]):\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        c_convs.append(c)\n",
        "    c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "    c_feat = Dropout(char_conf.get(\"dropout\", CFG[\"char\"].get(\"dropout\", 0.5)))(c_feat)\n",
        "\n",
        "    merged = concatenate([w_feat, c_feat])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "    return model\n",
        "\n",
        "def build_fasttext_combined_cnn(ft_embed_dim, word_max_len, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_max_len, ft_embed_dim), name=\"word_vec_input\")\n",
        "    convs = []\n",
        "    for f in CFG[\"cnn\"][\"filters\"]:\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_in)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    w_feat = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "    w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_convs = []\n",
        "    for f in char_conf.get(\"filters\", [3,4,5]):\n",
        "        c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "        c = GlobalMaxPooling1D()(c)\n",
        "        c_convs.append(c)\n",
        "    c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "    c_feat = Dropout(char_conf.get(\"dropout\", CFG[\"char\"].get(\"dropout\", 0.5)))(c_feat)\n",
        "\n",
        "    merged = concatenate([w_feat, c_feat])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "    return model\n",
        "\n",
        "# RNN builders\n",
        "def build_word_rnn(max_words, max_len, embedding_dim, n_classes, rnn_units, dropout):\n",
        "    inp = Input(shape=(max_len,), name=\"word_input\")\n",
        "    emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len)(inp)\n",
        "    x = Bidirectional(LSTM(rnn_units))(emb)\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "    return model\n",
        "\n",
        "def build_char_rnn(vocab_size, max_chars, embedding_dim, rnn_units, dropout, n_classes):\n",
        "    inp = Input(shape=(max_chars,), name=\"char_input\")\n",
        "    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_chars)(inp)\n",
        "    x = Bidirectional(LSTM(rnn_units))(emb)\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "    return model\n",
        "\n",
        "def build_combined_rnn(word_conf, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_conf[\"max_len\"],), name=\"word_input\")\n",
        "    w_emb = Embedding(input_dim=word_conf[\"max_words\"], output_dim=word_conf[\"embedding_dim\"], input_length=word_conf[\"max_len\"])(w_in)\n",
        "    w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_emb)\n",
        "    w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_x = Bidirectional(LSTM(char_conf.get(\"rnn_units\", CFG[\"rnn\"][\"rnn_units\"]))(c_emb))\n",
        "    c_x = Dropout(char_conf.get(\"dropout\", CFG[\"rnn\"][\"dropout\"]))(c_x)\n",
        "\n",
        "    merged = concatenate([w_x, c_x])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "    return model\n",
        "\n",
        "def build_fasttext_combined_rnn(ft_embed_dim, word_max_len, char_conf, n_classes):\n",
        "    w_in = Input(shape=(word_max_len, ft_embed_dim), name=\"word_vec_input\")\n",
        "    w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_in)\n",
        "    w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "    c_in = Input(shape=(char_conf[\"max_chars\"],), name=\"char_input\")\n",
        "    c_emb = Embedding(input_dim=char_conf[\"vocab_size\"], output_dim=char_conf[\"embedding_dim\"], input_length=char_conf[\"max_chars\"])(c_in)\n",
        "    c_x = Bidirectional(LSTM(char_conf.get(\"rnn_units\", CFG[\"rnn\"][\"rnn_units\"]))(c_emb))\n",
        "    c_x = Dropout(char_conf.get(\"dropout\", CFG[\"rnn\"][\"dropout\"]))(c_x)\n",
        "\n",
        "    merged = concatenate([w_x, c_x])\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "    model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# Central training function\n",
        "# -----------------------------\n",
        "def train_model(model_name, csv_path=\"\", force_retrain=False):\n",
        "    model_name = str(model_name).strip()\n",
        "    if model_name not in MODEL_NAMES:\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "    append_log(model_name, f\"=== START TRAIN [{model_name}] ===\")\n",
        "    print(f\"Starting training for: {model_name}\")\n",
        "\n",
        "    paths = model_paths(model_name)\n",
        "    df = load_data(csv_path)\n",
        "    texts = df[\"trade_name\"].tolist()\n",
        "\n",
        "    # labels\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(df[\"reason\"])\n",
        "    classes = le.classes_\n",
        "    n_classes = len(classes)\n",
        "    np.save(paths[\"classes\"], classes, allow_pickle=True)\n",
        "    y_cat = to_categorical(y, num_classes=n_classes)\n",
        "\n",
        "    state = load_training_state(model_name) or {}\n",
        "    phases = state.get(\"phases\", {})\n",
        "    last_completed = int(phases.get(model_name, {}).get(\"last_completed_epoch\", 0))\n",
        "    initial_epoch = last_completed\n",
        "    epochs = CFG[\"training\"][\"epochs\"]\n",
        "\n",
        "    cb_epoch = ModelCheckpoint(paths[\"model_epoch_pattern\"], save_best_only=False, monitor=\"val_loss\", mode=\"min\", verbose=0) # verbose=0\n",
        "    cb_best = ModelCheckpoint(paths[\"model_best\"], save_best_only=True, monitor=\"val_loss\", mode=\"min\", verbose=0) # verbose=0\n",
        "    cb_early = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=0) # verbose=0\n",
        "    cb_state = EpochCheckpointCallback(model_name)\n",
        "    cb_batch_metrics = BatchMetricsCallback() # Instantiate custom batch metrics callback\n",
        "\n",
        "    callbacks_list = [cb_epoch, cb_best, cb_early, cb_state, cb_batch_metrics]\n",
        "\n",
        "    try:\n",
        "        # 1) cnn_word\n",
        "        if model_name == \"cnn_word\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                model = build_word_cnn(CFG[\"word\"][\"max_words\"], CFG[\"word\"][\"max_len\"], CFG[\"word\"][\"embedding_dim\"], n_classes, CFG[\"cnn\"][\"filters\"], CFG[\"cnn\"][\"num_filters\"], CFG[\"cnn\"][\"dropout\"])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_word, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 2) cnn_char\n",
        "        elif model_name == \"cnn_char\":\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                filters = CFG[\"char\"].get(\"filters\", [3,4,5])\n",
        "                model = build_char_cnn(vocab_size, CFG[\"char\"][\"max_chars\"], CFG[\"char\"][\"embedding_dim\"], filters, CFG[\"cnn\"][\"num_filters\"], CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"]), n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_char, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 3) cnn_combined\n",
        "        elif model_name == \"cnn_combined\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                word_conf = {\"max_len\": CFG[\"word\"][\"max_len\"], \"max_words\": CFG[\"word\"][\"max_words\"], \"embedding_dim\": CFG[\"word\"][\"embedding_dim\"]}\n",
        "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"filters\": CFG[\"char\"].get(\"filters\",[3,4,5]), \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])}\n",
        "                model = build_combined_cnn(word_conf, char_conf, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 4) cnn_fasttext_keras\n",
        "        elif model_name == \"cnn_fasttext_keras\":\n",
        "            # Keras-style fasttext: we use a trainable Embedding on word indices (no gensim)\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            vocab_size = min(CFG[\"word\"][\"max_words\"], (len(word_tok.word_index) + 1))\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            # char tokenizer for combined branch\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, _ = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                save_char_tokenizer(char_map, max(char_map.values())+1, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            # Model: word branch uses embedding with embedding_dim = fasttext_keras embedding dim\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                # Build CNN that uses trainable embedding for words (fasttext-like)\n",
        "                w_in = Input(shape=(CFG[\"word\"][\"max_len\"],), name=\"word_input\")\n",
        "                w_emb = Embedding(input_dim=vocab_size, output_dim=CFG[\"fasttext_keras\"][\"embedding_dim\"], input_length=CFG[\"word\"][\"max_len\"])(w_in)\n",
        "                convs = []\n",
        "                for f in CFG[\"cnn\"][\"filters\"]:\n",
        "                    c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(w_emb)\n",
        "                    c = GlobalMaxPooling1D()(c)\n",
        "                    convs.append(c)\n",
        "                w_feat = concatenate(convs) if len(convs) > 1 else convs[0]\n",
        "                w_feat = Dropout(CFG[\"cnn\"][\"dropout\"])(w_feat)\n",
        "\n",
        "                # char branch\n",
        "                c_in = Input(shape=(CFG[\"char\"][\"max_chars\"],), name=\"char_input\")\n",
        "                c_emb = Embedding(input_dim=max(char_map.values())+1, output_dim=CFG[\"char\"][\"embedding_dim\"], input_length=CFG[\"char\"][\"max_chars\"])(c_in)\n",
        "                c_convs = []\n",
        "                for f in CFG[\"char\"].get(\"filters\",[3,4,5]):\n",
        "                    c = Conv1D(filters=CFG[\"cnn\"][\"num_filters\"], kernel_size=f, activation=\"relu\")(c_emb)\n",
        "                    c = GlobalMaxPooling1D()(c)\n",
        "                    c_convs.append(c)\n",
        "                c_feat = concatenate(c_convs) if len(c_convs) > 1 else c_convs[0]\n",
        "                c_feat = Dropout(CFG[\"char\"].get(\"dropout\", 0.5))(c_feat)\n",
        "\n",
        "                merged = concatenate([w_feat, c_feat])\n",
        "                merged = Dropout(0.5)(merged)\n",
        "                out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "                model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 5) cnn_fasttext_gensim\n",
        "        elif model_name == \"cnn_fasttext_gensim\":\n",
        "            ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
        "            sentences = [tokenize_for_fasttext(t) for t in texts]\n",
        "            if os.path.exists(ft_path) and not force_retrain:\n",
        "                ft = load_fasttext_gensim(ft_path)\n",
        "            else:\n",
        "                ft = train_fasttext_gensim(sentences, ft_path, CFG[\"fasttext_gensim\"])\n",
        "            embed_dim = ft.vector_size\n",
        "            max_len = CFG[\"word\"][\"max_len\"]\n",
        "            X_word = np.stack([text_to_word_vectors(ft, tokenize_for_fasttext(t), max_len) for t in texts], axis=0)\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"filters\": CFG[\"char\"].get(\"filters\",[3,4,5]), \"dropout\": CFG[\"char\"].get(\"dropout\", CFG[\"cnn\"][\"dropout\"])}\n",
        "                model = build_fasttext_combined_cnn(embed_dim, CFG[\"word\"][\"max_len\"], char_conf, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 6) rnn_word\n",
        "        elif model_name == \"rnn_word\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                model = build_word_rnn(CFG[\"word\"][\"max_words\"], CFG[\"word\"][\"max_len\"], CFG[\"word\"][\"embedding_dim\"], n_classes, CFG[\"rnn\"][\"rnn_units\"], CFG[\"rnn\"][\"dropout\"])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_word, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 7) rnn_char\n",
        "        elif model_name == \"rnn_char\":\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                model = build_char_rnn(vocab_size, CFG[\"char\"][\"max_chars\"], CFG[\"char\"][\"embedding_dim\"], CFG[\"rnn\"][\"rnn_units\"], CFG[\"rnn\"][\"dropout\"], n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit(X_char, y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 8) rnn_combined\n",
        "        elif model_name == \"rnn_combined\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                word_conf = {\"max_len\": CFG[\"word\"][\"max_len\"], \"max_words\": CFG[\"word\"][\"max_words\"], \"embedding_dim\": CFG[\"word\"][\"embedding_dim\"]}\n",
        "                char_conf = {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"rnn_units\": CFG[\"rnn\"][\"rnn_units\"], \"dropout\": CFG[\"rnn\"][\"dropout\"]}\n",
        "                model = build_combined_rnn(word_conf, char_conf, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 9) rnn_fasttext_keras\n",
        "        elif model_name == \"rnn_fasttext_keras\":\n",
        "            tok_path = paths[\"word_tokenizer\"]\n",
        "            if os.path.exists(tok_path) and not force_retrain:\n",
        "                word_tok = load_tokenizer_json(tok_path)\n",
        "            else:\n",
        "                word_tok = build_word_tokenizer(texts, CFG[\"word\"][\"max_words\"])\n",
        "                save_tokenizer_json(word_tok, tok_path)\n",
        "            seqs = word_tok.texts_to_sequences(texts)\n",
        "            vocab_size = min(CFG[\"word\"][\"max_words\"], (len(word_tok.word_index) + 1))\n",
        "            X_word = pad_sequences(seqs, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, _ = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                save_char_tokenizer(char_map, max(char_map.values())+1, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                # RNN combined with trainable embedding for words\n",
        "                w_in = Input(shape=(CFG[\"word\"][\"max_len\"],), name=\"word_input\")\n",
        "                w_emb = Embedding(input_dim=vocab_size, output_dim=CFG[\"fasttext_keras\"][\"embedding_dim\"], input_length=CFG[\"word\"][\"max_len\"])(w_in)\n",
        "                w_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(w_emb)\n",
        "                w_x = Dropout(CFG[\"rnn\"][\"dropout\"])(w_x)\n",
        "\n",
        "                c_in = Input(shape=(CFG[\"char\"][\"max_chars\"],), name=\"char_input\")\n",
        "                c_emb = Embedding(input_dim=max(char_map.values())+1, output_dim=CFG[\"char\"][\"embedding_dim\"], input_length=CFG[\"char\"][\"max_chars\"])(c_in)\n",
        "                c_x = Bidirectional(LSTM(CFG[\"rnn\"][\"rnn_units\"]))(c_emb)\n",
        "                c_x = Dropout(CFG[\"rnn\"][\"dropout\"])(c_x)\n",
        "\n",
        "                merged = concatenate([w_x, c_x])\n",
        "                merged = Dropout(0.5)(merged)\n",
        "                out = Dense(n_classes, activation=\"softmax\")(merged)\n",
        "                model = Model(inputs=[w_in, c_in], outputs=out)\n",
        "                model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", precision, recall, f1_score])\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        # 10) rnn_fasttext_gensim\n",
        "        elif model_name == \"rnn_fasttext_gensim\":\n",
        "            ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
        "            sentences = [tokenize_for_fasttext(t) for t in texts]\n",
        "            if os.path.exists(ft_path) and not force_retrain:\n",
        "                ft = load_fasttext_gensim(ft_path)\n",
        "            else:\n",
        "                ft = train_fasttext_gensim(sentences, ft_path, CFG[\"fasttext_gensim\"])\n",
        "            embed_dim = ft.vector_size\n",
        "            max_len = CFG[\"word\"][\"max_len\"]\n",
        "            X_word = np.stack([text_to_word_vectors(ft, tokenize_for_fasttext(t), max_len) for t in texts], axis=0)\n",
        "            char_path = paths[\"char_tokenizer\"]\n",
        "            if os.path.exists(char_path) and not force_retrain:\n",
        "                char_map, vocab_size = load_char_tokenizer(char_path)\n",
        "            else:\n",
        "                char_map = build_char_tokenizer(texts, CFG[\"char\"][\"vocab_size\"])\n",
        "                vocab_size = max(char_map.values()) + 1\n",
        "                save_char_tokenizer(char_map, vocab_size, char_path)\n",
        "            X_char = texts_to_char_sequences(texts, char_map, CFG[\"char\"][\"max_chars\"])\n",
        "            if os.path.exists(paths[\"model_best\"]) and not force_retrain:\n",
        "                model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "            else:\n",
        "                model = build_fasttext_combined_rnn(embed_dim, CFG[\"word\"][\"max_len\"], {\"max_chars\": CFG[\"char\"][\"max_chars\"], \"vocab_size\": vocab_size, \"embedding_dim\": CFG[\"char\"][\"embedding_dim\"], \"rnn_units\": CFG[\"rnn\"][\"rnn_units\"], \"dropout\": CFG[\"rnn\"][\"dropout\"]}, n_classes)\n",
        "            if initial_epoch < epochs:\n",
        "                model.fit([X_word, X_char], y_cat, batch_size=CFG[\"training\"][\"batch_size\"], epochs=epochs, initial_epoch=initial_epoch, validation_split=CFG[\"training\"][\"validation_split\"], callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unhandled model: \" + model_name)\n",
        "\n",
        "    except Exception as exc:\n",
        "        tb = traceback.format_exc()\n",
        "        append_log(model_name, f\"TRAIN ERROR: {str(exc)}\\n{tb}\")\n",
        "        print(\"Error training\", model_name, \":\", str(exc))\n",
        "        raise\n",
        "\n",
        "    append_log(model_name, f\"=== FINISHED TRAIN [{model_name}] ===\")\n",
        "    print(f\"Training finished for: {model_name}\")\n",
        "    return True\n",
        "\n",
        "# -----------------------------\n",
        "# Resources & prediction\n",
        "# -----------------------------\n",
        "def load_resources_for_model(model_name):\n",
        "    if model_name not in MODEL_NAMES:\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "    paths = model_paths(model_name)\n",
        "    if not os.path.exists(paths[\"classes\"]):\n",
        "        raise FileNotFoundError(\"Classes file missing. Train first.\")\n",
        "    classes = np.load(paths[\"classes\"], allow_pickle=True)\n",
        "    if not os.path.exists(paths[\"model_best\"]):\n",
        "        raise FileNotFoundError(\"Best model missing. Train first.\")\n",
        "    # Pass custom objects when loading the model\n",
        "    model = load_model(paths[\"model_best\"], custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "    res = {\"classes\": classes, \"model\": model}\n",
        "\n",
        "    if model_name in [\"cnn_word\", \"rnn_word\", \"cnn_combined\", \"rnn_combined\", \"cnn_fasttext_keras\", \"rnn_fasttext_keras\"]:\n",
        "        if not os.path.exists(paths[\"word_tokenizer\"]):\n",
        "            raise FileNotFoundError(\"Word tokenizer missing.\")\n",
        "        res[\"word_tokenizer\"] = load_tokenizer_json(paths[\"word_tokenizer\"])\n",
        "\n",
        "    if model_name in [\"cnn_char\", \"rnn_char\", \"cnn_combined\", \"rnn_combined\", \"cnn_fasttext_keras\", \"cnn_fasttext_gensim\", \"rnn_fasttext_keras\", \"rnn_fasttext_gensim\"]:\n",
        "        if not os.path.exists(paths[\"char_tokenizer\"]):\n",
        "            raise FileNotFoundError(\"Char tokenizer missing.\")\n",
        "        char_map, _ = load_char_tokenizer(paths[\"char_tokenizer\"])\n",
        "        res[\"char_map\"] = char_map\n",
        "\n",
        "    if model_name in [\"cnn_fasttext_gensim\", \"rnn_fasttext_gensim\", \"cnn_fasttext_gensim\"]:\n",
        "        ft_path = model_paths(model_name)[\"fasttext_gensim\"]\n",
        "        if not os.path.exists(ft_path):\n",
        "            raise FileNotFoundError(\"FastText gensim model missing.\")\n",
        "        res[\"fasttext\"] = load_fasttext_gensim(ft_path)\n",
        "\n",
        "    return res\n",
        "\n",
        "def predict_for_model(model_name, text, resources):\n",
        "    t = simple_clean_text(text)\n",
        "    model = resources[\"model\"]\n",
        "    classes = resources[\"classes\"]\n",
        "    if model_name in [\"cnn_word\", \"rnn_word\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        x = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        preds = model.predict(x, verbose=0)\n",
        "    elif model_name in [\"cnn_char\", \"rnn_char\"]:\n",
        "        cm = resources[\"char_map\"]\n",
        "        x = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict(x, verbose=0)\n",
        "    elif model_name in [\"cnn_combined\", \"rnn_combined\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        xw = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    elif model_name in [\"cnn_fasttext_keras\", \"rnn_fasttext_keras\"]:\n",
        "        tok = resources[\"word_tokenizer\"]\n",
        "        seq = tok.texts_to_sequences([t])\n",
        "        xw = pad_sequences(seq, maxlen=CFG[\"word\"][\"max_len\"], padding=\"post\", truncating=\"post\")\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    elif model_name in [\"cnn_fasttext_gensim\", \"rnn_fasttext_gensim\"]:\n",
        "        ft = resources[\"fasttext\"]\n",
        "        tokens = tokenize_for_fasttext(t)\n",
        "        xw = np.expand_dims(text_to_word_vectors(ft, tokens, CFG[\"word\"][\"max_len\"]), axis=0)\n",
        "        cm = resources[\"char_map\"]\n",
        "        xc = texts_to_char_sequences([t], cm, CFG[\"char\"][\"max_chars\"])\n",
        "        preds = model.predict([xw, xc], verbose=0)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model: \" + model_name)\n",
        "\n",
        "    idx = int(np.argmax(preds, axis=1)[0])\n",
        "    prob = float(np.max(preds))\n",
        "    label = str(classes[idx])\n",
        "\n",
        "    # Clean the predicted label for comparison by removing Arabic semicolon and stripping whitespace\n",
        "    cleaned_label_for_comparison = label.replace('', '').strip()\n",
        "\n",
        "    # Check for both possible spellings of 'normal' based on user's input for acceptance\n",
        "    accepted_forms = [\"Normal\", \"\"] # Corrected to match the actual accepted form and user's intent if 'normal' is intended\n",
        "    accepted = cleaned_label_for_comparison in accepted_forms\n",
        "\n",
        "    return {\"label\": label, \"probability\": prob, \"accepted\": accepted}\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI\n",
        "# -----------------------------\n",
        "def build_gradio():\n",
        "    def on_train(model_name, csv_path, force):\n",
        "        try:\n",
        "            train_model(model_name, csv_path=csv_path.strip() if csv_path else \"\", force_retrain=force)\n",
        "            return f\"Training finished for: {model_name}\"\n",
        "        except Exception as e:\n",
        "            return \"ERROR: \" + str(e)\n",
        "\n",
        "    def on_predict(model_name, name):\n",
        "        try:\n",
        "            res = load_resources_for_model(model_name)\n",
        "            r = predict_for_model(model_name, name, res)\n",
        "            label = r[\"label\"]\n",
        "            conf = f\"{r['probability']*100:.2f}%\"\n",
        "            st = \"ACCEPTED \" if r[\"accepted\"] else \"REJECTED \"\n",
        "            return label, conf, st\n",
        "        except Exception as e:\n",
        "            return \"ERROR: \" + str(e), \"\", \"\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Unified Models  select a model, train/resume, or predict\")\n",
        "        with gr.Row():\n",
        "            model_select = gr.Dropdown(MODEL_NAMES, value=MODEL_NAMES[0], label=\"Model\")\n",
        "            csv_input = gr.Textbox(label=\"CSV path (leave empty to use fallback)\", value=\"\")\n",
        "        with gr.Row():\n",
        "            train_btn = gr.Button(\"Train / Resume Selected Model\")\n",
        "            force_chk = gr.Checkbox(label=\"Force rebuild (delete/load fresh)\", value=False)\n",
        "            status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "        with gr.Row():\n",
        "            name_input = gr.Textbox(label=\"Proposed Trade Name\")\n",
        "            predict_btn = gr.Button(\"Predict\")\n",
        "        with gr.Row():\n",
        "            out_label = gr.Textbox(label=\"Predicted Reason\")\n",
        "            out_conf = gr.Textbox(label=\"Confidence\")\n",
        "            out_status = gr.Textbox(label=\"Decision\")\n",
        "\n",
        "        train_btn.click(on_train, inputs=[model_select, csv_input, force_chk], outputs=[status])\n",
        "        predict_btn.click(on_predict, inputs=[model_select, name_input], outputs=[out_label, out_conf, out_status])\n",
        "\n",
        "    return demo\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--csv', default='', help='Path to CSV dataset (optional)')\n",
        "    parser.add_argument('--train', type=str, help='Train a specific model (name)')\n",
        "    parser.add_argument('--force', action='store_true', help='Force rebuild')\n",
        "    parser.add_argument('--serve', action='store_true', help='Launch Gradio UI')\n",
        "    args = parser.parse_known_args()[0]\n",
        "\n",
        "    if args.train:\n",
        "        print(\"Training:\", args.train)\n",
        "        train_model(args.train, csv_path=args.csv, force_retrain=args.force)\n",
        "    elif args.serve:\n",
        "        demo = build_gradio()\n",
        "        demo.launch()\n",
        "    else:\n",
        "        print(\"Script ready. Use --train <model_name> or --serve to launch the UI.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # The previous cell had an error and interrupted the execution. This ensures the main is called properly if not handled by a direct train_model call.\n",
        "    # sys.argv=['rnn_fasttext_keras', '--serve'] # This line was causing issues, removing it\n",
        "    # If the intent is to serve the gradio UI, it should be called directly or via `main()` with appropriate arguments.\n",
        "    # For the purpose of this subtask, we are assuming direct training calls or the UI will handle it.\n",
        "    pass\n",
        "\n",
        "print(\"Model build functions and train_model updated with F1 metrics and BatchMetricsCallback, and verbose=0 for fit calls.\")"
      ],
      "id": "4f758e7e",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Model build functions and train_model updated with F1 metrics and BatchMetricsCallback, and verbose=0 for fit calls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed sys.argv modification and direct call to main() due to unexpected behavior.\n",
        "# Calling train_model directly to ensure training starts for 'cnn_word'.\n",
        "# train_model('cnn_char') # Corrected from 'char_cnn'\n",
        "\n",
        "# train_model(\"cnn_word\")\n",
        "train_model(\"cnn_char\")\n",
        "# train_model(\"cnn_combined\")\n",
        "# train_model(\"cnn_fasttext_keras\")\n",
        "# train_model(\"cnn_fasttext_gensim\")\n",
        "# train_model(\"rnn_word\")\n",
        "# train_model(\"rnn_char\")\n",
        "# train_model(\"rnn_combined\")\n",
        "# train_model(\"rnn_fasttext_keras\")\n",
        "# train_model(\"rnn_fasttext_gensim\")\n",
        "\n",
        "# The model name 'serv' is not a valid model. Please choose from:\n",
        "# ['cnn_word', 'cnn_char', 'cnn_combined', 'cnn_fasttext_keras', 'cnn_fasttext_gensim',\n",
        "#  'rnn_word', 'rnn_char', 'rnn_combined', 'rnn_fasttext_keras', 'rnn_fasttext_gensim']\n",
        "#\n",
        "# If you meant to launch the Gradio UI, use the following code:\n",
        "# import sys\n",
        "sys.argv=['cnn_word', '--serve']\n",
        "main()"
      ],
      "metadata": {
        "id": "Yv4X1VAEIzc3",
        "outputId": "882a2722-465b-4474-96c5-e55fcfc6c799",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Yv4X1VAEIzc3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for: cnn_char\n",
            "Epoch 1 - Batch 6431 (train): accuracy: 0.9725 f1_score: 0.9710 loss: 0.0941 precision: 0.9819 recall: 0.9633"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}